\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
% (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
% \typeout{(#1)}
% \@addtofilelist{#1}
% \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
%   \makeatother
%   \newcommand*{\myexternaldocument}[1]{%
%   \externaldocument{#1}%
%   \addFileDependency{#1.tex}%
%   \addFileDependency{#1.aux}%
% }
%   \myexternaldocument{external_tex_file}
%   --- end of overleaf setting ---

%   mathtools can be used to define labeling format for equations
%   one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
% you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
% you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{Survival Analysis}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle
\tableofcontents{}

\section{Basic knowledge}
\label{sec:basic-knowledge}

\subsection{Survival and hazard}
\label{sec:survival-hazard}

Let $T$ denote the time to an event that we are interested in. Then we know the c.d.f.
\[
  F_T\left(t\right) = P\left(T \leq t\right)
  ,
\]
and the corresponding p.d.f.
\[
  f_T\left(t\right) = \frac{\mathrm{d}}{\mathrm{d}t}F_T\left(t\right)
  .
\]
Here to simplify the discussion, we assume $T$ is a continuous random variable. In the context of survival analysis, the \emph{event} often refers to death. Then $T$ represents the lifespan of the subject. So $F_T\left(t\right)$ represents the probability that the death occurs before $t$. In another word, we know the probability that the subject survives passes $t$ is
\[
  S_T\left(t\right) = 1 - F_T\left(t\right) = P\left(T > t\right)
  .
\]
$S_T\left(t\right)$ is often called the {\color{red} survival function?} and clearly
\[
  f_T\left(t\right) = - \frac{\mathrm{d}}{\mathrm{d}t}S_T\left(t\right)
  .
\]
The \textbf{hazard function} $h\left(t\right)$ is defined as
\[
  h\left(t\right) = \underset{\Delta\to0}{\mathrm{lim}}
  \frac{
    P\left(
      T \leq t + \Delta \middle| T > t
    \right)
  }{\Delta}
  = \underset{\Delta\to0}{\mathrm{lim}}
  \frac{
    F_T\left(t + \Delta\right) - F_T\left(t\right)
  }{\Delta \cdot S_T\left(t\right)}
  = \frac{f_T\left(t\right)}{S_T\left(t\right)}
  .
\]
$h\left(t\right)$ represents the {\color{red} instant hazard? unified probability?} that the subject will be dead instantly after $t$ given the fact that it's alive at $t$. And the \textbf{cummulative hazard function} is
\[
  H\left(t\right) = \int_0^th\left(x\right)\mathrm{d}x
  = \int_0^t \frac{f_T\left(x\right)}{S_T\left(x\right)}\mathrm{d}x
  = \int_0^t\frac{-\mathrm{d}S_T\left(x\right)}{S_X\left(t\right)}
  = \left. -\mathrm{log}\left(S_T\left(x\right)\right)\right|_0^t
  = - \mathrm{log}\left(S_T\left(t\right)\right)
  .
\]

\begin{prop}
  The random variable $H\left(T\right)$ follows unit exponential distribution $EXP(1)$.
\end{prop}
\begin{proof}
  \[
    \begin{aligned}
      P\left(H\left(T\right)\leq t\right)
      =& P\left(-\mathrm{log} S\left(T\right) \leq t\right)    \\
      =& P\left(1 - F\left(T\right) \geq e^{-t}\right)    \\
      =& P\left(T \leq F^{-1}\left(1 - e^{-t}\right)\right)    \\
      =& F\left(F^{-1}\left(1 - e^{-t}\right)\right)    \\
      =& 1 - e^{-t}
      , 
    \end{aligned}    
  \]
  which is the c.d.f of $EXP\left(1\right)$. Here to simplify the deduction we make some assumptions that
  \begin{itemize}
  \item $F\left(t\right)$ is continuous.
  \item $F^{-1}\left(t\right)$ is well defined.
  \end{itemize}
  Also to simplify the notation and avoid confusion, we use $S\left(\cdot\right)$ and $F\left(\cdot\right)$ instead of $S_T\left(\cdot\right)$ and $F_T\left(\cdot\right)$ like before.
\end{proof}

\begin{enumerate}
\item \textbf{Exponential distribution:} Denote $T\sim EXP(\lambda)$. Then
  \[
    \begin{aligned}
      & f\left(t\right) = \lambda e^{-\lambda t}    \\
      & F\left(t\right) = 1 - e^{-\lambda t}
      \quad\quad S\left(t\right) = e^{-\lambda t}    \\
      & h\left(t\right) = \lambda
      \quad\quad{\color{red}\text{constant hazard}}\\
      & H\left(t\right) = \lambda t    \\
      & \mathrm{E}\left(T\right) = 1 / \lambda
      \quad\quad \mathrm{Var}\left(T\right) = 1 / \lambda ^ 2
    \end{aligned}
  \]
  
\item \textbf{Weibull distribution:} Denote $T\sim W\left(p, \lambda\right)$. Then
  \[
    \begin{aligned}
      & f\left(t\right) = p\lambda^pt^{p - 1}\mathrm{exp}\left(-\left(\lambda t\right)^p\right)    \\
      & F\left(t\right) = 1 - \mathrm{exp}\left(-\left(\lambda t\right)^p\right)
      \quad\quad S\left(t\right) = \mathrm{exp}\left(-\left(\lambda t\right)^p\right)    \\
      & h\left(t\right) = p\lambda^pt^{p - 1}    \\
      & H\left(t\right) = \left(\lambda t\right)^p    \\
      & \mathrm{E}\left(T\right) = \frac{1}{\lambda} \cdot \Gamma\left(1 + \frac{1}{p}\right)
      \quad\quad \mathrm{Var}\left(T\right) = \frac{1}{\lambda^2}\left(
        \Gamma\left(1 + \frac{2}{p}\right)
        - \Gamma\left(1 + \frac{1}{p}\right)
      \right)    \\
      & \mathrm{E}\left(T^m\right) = \frac{1}{\lambda^m}\Gamma\left(1 + \frac{m}{p}\right)
    \end{aligned}
  \]
\end{enumerate}

\subsection{Censor}
\label{sec:censor}

\subsubsection{Right censor}
\label{sec:right-censor}

\begin{itemize}
\item Type I: an i.i.d sample $T_1, \cdots, T_n \sim F$ and a {\color{red} fixed} constant $c$. And the observed data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$ where
  \[
    \begin{aligned}
      & U_i = \mathrm{min}\left(T_i, c\right)    \\
      & \delta_i = 1_{T_i \leq c}
      .
    \end{aligned}
  \]
  So the observed data consists of a {\color{red} random} number, $r$, of uncensored observations, all of which are less than $c$. And $n - r$ censored observations, all are $c$.
  
\item Type II: an i.i.d sample $T_1, \cdots, T_n \sim F$ and a {\color{red} pre-defined} number of failure $r$. The observation is stopped when $r$ failure occures and the stopping time is $c$. The observed data is still the form $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, the same as that in Type I censor. But in actuality, we observe the first $r$ {\color{red} order statistics}
  \[
    T_{\left(1, n\right)}, \cdots, T_{\left(r, n\right)}
    .
  \]
  Note that here $\left(U_1, \delta_1\right), \cdots, \left(U_n, \delta_n\right)$ are {\color{red} dependent} whereas they are independent for Type I.
  
\item Type III (Random censor): The underlying data is
  \[
    \begin{aligned}
      & c_1, \cdots, c_n \quad \text{constant}    \\
      & T_1, \cdots, T_n \sim F
      .
    \end{aligned}
  \]
  And the observed data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, where
  \[
    \begin{aligned}
      & U_i = \mathrm{min}\left(T_i, c_i\right)    \\
      & \delta_i = 1_{T_i \leq c_i}
      .
    \end{aligned}
  \]
  \textbf{Note: } for inference, $c_i$ is often treated as constant. For study design or studying the asymptotic property, they are often treated as i.i.d random variables $C_1, \cdots, C_n$.
\end{itemize}

\subsubsection{Left censor}
\label{sec:left-censor}

$T_i$ is censored when $T_i \leq l_i$.

\subsubsection{Interval censor}
\label{sec:interval-censor}

$l_i \leq T_i \leq u_i$, but only $l_i$ and $u_i$ are observed.

\section{MLE}
\label{sec:mle}

There is an i.i.d survival time sample $T_1, \cdots, T_n$ with common and unknown c.d.f. $F\left(\cdot\right)$ and the observated data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, where
\[
  \begin{aligned}
    & U_i = \mathrm{min}\left(T_i, C_i\right)    \\
    & \delta_i = 1\left(T_i \leq C_i\right)
  \end{aligned}
\]
and $C_i$ is the ({\color{red} fixed} or {\color{red} random}) censoring time. Let $\bot$ denote ``is independent of''. We assume $T_i\bot C_i$ ({\color{red} Non-informative censoring, the key assumption}) and $\left(U_i, \delta_i\right)$ are also i.i.d. The observed data consists of two parts. $U_i$ is continuous while $\delta_i$ is binary.
\[
  \begin{aligned}
    & \left(U_i, \delta_i\right) = \left(u_i, 1\right)
    && \quad\quad T_i \text{ is uncensored at } u_i    \\
    & \left(U_i, \delta_i\right) = \left(u_i, 0\right)
    && \quad\quad T_i \text{ is censored at } u_i    \\
  \end{aligned}
\]

\textbf{When $C_i$s are known constants}, the likelihood for $\left(U_i, \delta_i\right)$ is
\[
  \begin{aligned}
      L_i\left(F\right) =& \left\{
    \begin{aligned}
      & f\left(u_i\right) && \quad \text{if } \delta_i = 1    \\
      & 1 - F\left(u_i\right) && \quad \text{if } \delta_i = 0
    \end{aligned}
  \right.    \\
  =& f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
  \end{aligned}
\]
Therefore
\begin{equation}
  \label{eq:likelihood_core}
  L\left(F\right) = \prod\limits_{i = 1}^nL_i\left(F\right)
  = \prod\limits_{i = 1}^n\left(
    f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
  \right)
  = \prod\limits_{i = 1}^n\left(
    h\left(u_i\right)^{\delta_i}S\left(u_i\right)
  \right)
  .
\end{equation}
The last equality relies on the fact that $f\left(t\right) = h\left(t\right)S\left(t\right)$.

\textbf{When $C_i$s are i.i.d. $\sim G$}, where $G$ is continuous with p.d.f $g$. Then we have
\[
  \begin{aligned}
    P\left(U_i \leq u, \delta_i = 1\right) = P\left(T_i \leq u, T_i \leq C_i\right)
    = \int_0^u\int_t^\infty f\left(t\right)g\left(c\right)\mathrm{d}c\mathrm{d}t
    = \int_0^uf\left(t\right)\left(1 - G\left(t\right)\right)\mathrm{d}t
  \end{aligned}
\]
Therefore the likelihood for $\delta_i = 1$ is
\[
  L_i\left(F, G\right) = f\left(u_i\right)\left(1 - G\left(u_i\right)\right)
  \quad\quad \text{ when } \delta_i = 1
  .
\]
And similarly, for $\delta_i = 0$, the likelihood is
\[
  L_i\left(F, G\right) = g\left(u_i\right)\left(1 - F\left(u_i\right)\right)
  \quad\quad \text{ when } \delta_i = 0
  .
\]
Hence the full likelihood is
\begin{equation}
  \label{eq:likelihood_full}
  \begin{aligned}
    L\left(F, G\right)
    =& \prod\limits_{i = 1}^n\left\{
      \left(f\left(u_i\right)\left(1 - G\left(u_i\right)\right)\right)^{\delta_i}
      \left(\left(1 - F\left(u_i\right)\right)g\left(u_i\right)\right)^{1 - \delta_i}
    \right\}    \\
    =& \prod\limits_{i = 1}^n\left\{
      f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
    \right\}
    \cdot \prod\limits_{i = 1}^n\left\{
      g\left(u_i\right)^{1 - \delta_i}\left(1 - G\left(u_i\right)\right)^{\delta_i}
    \right\}
  \end{aligned}
\end{equation}
So the core to maximize $L\left(F, G\right)$ with respect to $F$ in \eqref{eq:likelihood_full} is the same as that in \eqref{eq:likelihood_core}.

\subsection{Parametric MLE}
\label{sec:parametric-mle}

Suppose $T_1, \cdots, T_n$ are i.i.d. $Exp\left(\lambda\right)$, and subject to noninformative right censoring. Then \eqref{eq:likelihood_core} becomes

\[
  L = L\left(\lambda\right) =
  \prod\limits_{i = 1}^m\left\{
    \left(
      \lambda e^{-\lambda u_i}
    \right)^{\delta_i}
    \left(
      e^{-\lambda u_i}
    \right)^{1 - \delta_i}
  \right\}
  = \lambda^{\sum\limits_{i = 1}^n \delta_i}
  e^{-\lambda \sum\limits_{i = 1}^n u_i}
  = \lambda^r
  e^{-\lambda W}
  ,
\]
where $r = \sum\limits_{i = 1}^n \delta_i$ is the number of observed events and $W = \sum\limits_{i = 1}^n u_i$ is the total of observed time. Therefore $\mathrm{log}L = r\mathrm{log}\lambda - \lambda W$ and the MLE for $\lambda$ is
\[
  \hat{\lambda} = \frac{r}{W}
  .
\]
Furthermore, we know that
\[
  \left.
    \begin{aligned}
      \frac{\partial \mathrm{log}L}{\partial \lambda} &= \frac{r}{\lambda} - W    \\
      \frac{\partial^2 \mathrm{log}L}{\partial \lambda^2} &= - \frac{r}{\lambda^2}
    \end{aligned}
  \right\}
  \implies
\]

\subsection{Nonparametric MLE}
\label{sec:nonparametric-mle}




\bibliographystyle{plainnat}
\bibliography{../ref}





\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
