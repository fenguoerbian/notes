\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
% (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
% \typeout{(#1)}
% \@addtofilelist{#1}
% \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
%   \makeatother
%   \newcommand*{\myexternaldocument}[1]{%
%   \externaldocument{#1}%
%   \addFileDependency{#1.tex}%
%   \addFileDependency{#1.aux}%
% }
%   \myexternaldocument{external_tex_file}
%   --- end of overleaf setting ---

%   mathtools can be used to define labeling format for equations
%   one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
% you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
% you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{Survival Analysis}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle
\tableofcontents{}

\section{Basic knowledge}
\label{sec:basic-knowledge}

\subsection{Survival and hazard}
\label{sec:survival-hazard}

Let $T$ denote the time to an event that we are interested in. Then we know the c.d.f.
\[
  F_T\left(t\right) = P\left(T \leq t\right)
  ,
\]
and the corresponding p.d.f.
\[
  f_T\left(t\right) = \frac{\mathrm{d}}{\mathrm{d}t}F_T\left(t\right)
  .
\]
Here to simplify the discussion, we assume $T$ is a continuous random variable. In the context of survival analysis, the \emph{event} often refers to death. Then $T$ represents the lifespan of the subject. So $F_T\left(t\right)$ represents the probability that the death occurs before $t$. In another word, we know the probability that the subject survives passes $t$ is
\[
  S_T\left(t\right) = 1 - F_T\left(t\right) = P\left(T > t\right)
  .
\]
$S_T\left(t\right)$ is often called the {\color{red} survival function?} and clearly
\[
  f_T\left(t\right) = - \frac{\mathrm{d}}{\mathrm{d}t}S_T\left(t\right)
  .
\]
The \textbf{hazard function} $h\left(t\right)$ is defined as
\begin{equation}
  \label{eq:def_hazard_function}
    h\left(t\right) = \underset{\Delta\to0}{\mathrm{lim}}
  \frac{
    P\left(
      T \leq t + \Delta \middle| T > t
    \right)
  }{\Delta}
  = \underset{\Delta\to0}{\mathrm{lim}}
  \frac{
    F_T\left(t + \Delta\right) - F_T\left(t\right)
  }{\Delta \cdot S_T\left(t\right)}
  = \frac{f_T\left(t\right)}{S_T\left(t\right)}
  .
\end{equation}
$h\left(t\right)$ represents the {\color{red} instant hazard? unified probability?} that the subject will be dead instantly after $t$ given the fact that it's alive at $t$. And the \textbf{cummulative hazard function} is
\[
  H\left(t\right) = \int_0^th\left(x\right)\mathrm{d}x
  = \int_0^t \frac{f_T\left(x\right)}{S_T\left(x\right)}\mathrm{d}x
  = \int_0^t\frac{-\mathrm{d}S_T\left(x\right)}{S_X\left(t\right)}
  = \left. -\mathrm{log}\left(S_T\left(x\right)\right)\right|_0^t
  = - \mathrm{log}\left(S_T\left(t\right)\right)
  .
\]

\begin{prop}
  The random variable $H\left(T\right)$ follows unit exponential distribution $EXP(1)$.
\end{prop}
\begin{proof}
  \[
    \begin{aligned}
      P\left(H\left(T\right)\leq t\right)
      =& P\left(-\mathrm{log} S\left(T\right) \leq t\right)    \\
      =& P\left(1 - F\left(T\right) \geq e^{-t}\right)    \\
      =& P\left(T \leq F^{-1}\left(1 - e^{-t}\right)\right)    \\
      =& F\left(F^{-1}\left(1 - e^{-t}\right)\right)    \\
      =& 1 - e^{-t}
      , 
    \end{aligned}    
  \]
  which is the c.d.f of $EXP\left(1\right)$. Here to simplify the deduction we make some assumptions that
  \begin{itemize}
  \item $F\left(t\right)$ is continuous.
  \item $F^{-1}\left(t\right)$ is well defined.
  \end{itemize}
  Also to simplify the notation and avoid confusion, we use $S\left(\cdot\right)$ and $F\left(\cdot\right)$ instead of $S_T\left(\cdot\right)$ and $F_T\left(\cdot\right)$ like before.
\end{proof}

\begin{enumerate}
\item \textbf{Exponential distribution:} Denote $T\sim EXP(\lambda)$. Then
  \[
    \begin{aligned}
      & f\left(t\right) = \lambda e^{-\lambda t}    \\
      & F\left(t\right) = 1 - e^{-\lambda t}
      \quad\quad S\left(t\right) = e^{-\lambda t}    \\
      & h\left(t\right) = \lambda
      \quad\quad{\color{red}\text{constant hazard}}\\
      & H\left(t\right) = \lambda t    \\
      & \mathrm{E}\left(T\right) = 1 / \lambda
      \quad\quad \mathrm{Var}\left(T\right) = 1 / \lambda ^ 2
    \end{aligned}
  \]
  
\item \textbf{Weibull distribution:} Denote $T\sim W\left(p, \lambda\right)$. Then
  \[
    \begin{aligned}
      & f\left(t\right) = p\lambda^pt^{p - 1}\mathrm{exp}\left(-\left(\lambda t\right)^p\right)    \\
      & F\left(t\right) = 1 - \mathrm{exp}\left(-\left(\lambda t\right)^p\right)
      \quad\quad S\left(t\right) = \mathrm{exp}\left(-\left(\lambda t\right)^p\right)    \\
      & h\left(t\right) = p\lambda^pt^{p - 1}    \\
      & H\left(t\right) = \left(\lambda t\right)^p    \\
      & \mathrm{E}\left(T\right) = \frac{1}{\lambda} \cdot \Gamma\left(1 + \frac{1}{p}\right)
      \quad\quad \mathrm{Var}\left(T\right) = \frac{1}{\lambda^2}\left(
        \Gamma\left(1 + \frac{2}{p}\right)
        - \Gamma\left(1 + \frac{1}{p}\right)
      \right)    \\
      & \mathrm{E}\left(T^m\right) = \frac{1}{\lambda^m}\Gamma\left(1 + \frac{m}{p}\right)
    \end{aligned}
  \]
\end{enumerate}

\subsection{Censor}
\label{sec:censor}

\subsubsection{Right censor}
\label{sec:right-censor}

\begin{itemize}
\item Type I: an i.i.d sample $T_1, \cdots, T_n \sim F$ and a {\color{red} fixed} constant $c$. And the observed data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$ where
  \[
    \begin{aligned}
      & U_i = \mathrm{min}\left(T_i, c\right)    \\
      & \delta_i = 1_{T_i \leq c}
      .
    \end{aligned}
  \]
  So the observed data consists of a {\color{red} random} number, $r$, of uncensored observations, all of which are less than $c$. And $n - r$ censored observations, all are $c$.
  
\item Type II: an i.i.d sample $T_1, \cdots, T_n \sim F$ and a {\color{red} pre-defined} number of failure $r$. The observation is stopped when $r$ failure occures and the stopping time is $c$. The observed data is still the form $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, the same as that in Type I censor. But in actuality, we observe the first $r$ {\color{red} order statistics}
  \[
    T_{\left(1, n\right)}, \cdots, T_{\left(r, n\right)}
    .
  \]
  Note that here $\left(U_1, \delta_1\right), \cdots, \left(U_n, \delta_n\right)$ are {\color{red} dependent} whereas they are independent for Type I.
  
\item Type III (Random censor): The underlying data is
  \[
    \begin{aligned}
      & c_1, \cdots, c_n \quad \text{constant}    \\
      & T_1, \cdots, T_n \sim F
      .
    \end{aligned}
  \]
  And the observed data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, where
  \[
    \begin{aligned}
      & U_i = \mathrm{min}\left(T_i, c_i\right)    \\
      & \delta_i = 1_{T_i \leq c_i}
      .
    \end{aligned}
  \]
  \textbf{Note: } for inference, $c_i$ is often treated as constant. For study design or studying the asymptotic property, they are often treated as i.i.d random variables $C_1, \cdots, C_n$.
\end{itemize}

\subsubsection{Left censor}
\label{sec:left-censor}

$T_i$ is censored when $T_i \leq l_i$.

\subsubsection{Interval censor}
\label{sec:interval-censor}

$l_i \leq T_i \leq u_i$, but only $l_i$ and $u_i$ are observed.

\section{MLE}
\label{sec:mle}

There is an i.i.d survival time sample $T_1, \cdots, T_n$ with common and unknown c.d.f. $F\left(\cdot\right)$ and the observated data is $\left(U_i, \delta_i\right)$ for $i = 1, \cdots, n$, where
\[
  \begin{aligned}
    & U_i = \mathrm{min}\left(T_i, C_i\right)    \\
    & \delta_i = 1\left(T_i \leq C_i\right)
  \end{aligned}
\]
and $C_i$ is the ({\color{red} fixed} or {\color{red} random}) censoring time. Let $\bot$ denote ``is independent of''. We assume $T_i\bot C_i$ ({\color{red} Non-informative censoring, the key assumption}) and $\left(U_i, \delta_i\right)$ are also i.i.d. The observed data consists of two parts. $U_i$ is continuous while $\delta_i$ is binary.
\[
  \begin{aligned}
    & \left(U_i, \delta_i\right) = \left(u_i, 1\right)
    && \quad\quad T_i \text{ is uncensored at } u_i    \\
    & \left(U_i, \delta_i\right) = \left(u_i, 0\right)
    && \quad\quad T_i \text{ is censored at } u_i    \\
  \end{aligned}
\]

\textbf{When $C_i$s are known constants}, the likelihood for $\left(U_i, \delta_i\right)$ is
\[
  \begin{aligned}
      L_i\left(F\right) =& \left\{
    \begin{aligned}
      & f\left(u_i\right) && \quad \text{if } \delta_i = 1    \\
      & 1 - F\left(u_i\right) && \quad \text{if } \delta_i = 0
    \end{aligned}
  \right.    \\
  =& f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
  \end{aligned}
\]
Therefore
\begin{equation}
  \label{eq:likelihood_core}
  L\left(F\right) = \prod\limits_{i = 1}^nL_i\left(F\right)
  = \prod\limits_{i = 1}^n\left(
    f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
  \right)
  = \prod\limits_{i = 1}^n\left(
    h\left(u_i\right)^{\delta_i}S\left(u_i\right)
  \right)
  .
\end{equation}
The last equality relies on the fact that $f\left(t\right) = h\left(t\right)S\left(t\right)$.

\textbf{When $C_i$s are i.i.d. $\sim G$}, where $G$ is continuous with p.d.f $g$. Then we have
\[
  \begin{aligned}
    P\left(U_i \leq u, \delta_i = 1\right) = P\left(T_i \leq u, T_i \leq C_i\right)
    = \int_0^u\int_t^\infty f\left(t\right)g\left(c\right)\mathrm{d}c\mathrm{d}t
    = \int_0^uf\left(t\right)\left(1 - G\left(t\right)\right)\mathrm{d}t
  \end{aligned}
\]
Therefore the likelihood for $\delta_i = 1$ is
\[
  L_i\left(F, G\right) = f\left(u_i\right)\left(1 - G\left(u_i\right)\right)
  \quad\quad \text{ when } \delta_i = 1
  .
\]
And similarly, for $\delta_i = 0$, the likelihood is
\[
  L_i\left(F, G\right) = g\left(u_i\right)\left(1 - F\left(u_i\right)\right)
  \quad\quad \text{ when } \delta_i = 0
  .
\]
Hence the full likelihood is
\begin{equation}
  \label{eq:likelihood_full}
  \begin{aligned}
    L\left(F, G\right)
    =& \prod\limits_{i = 1}^n\left\{
      \left(f\left(u_i\right)\left(1 - G\left(u_i\right)\right)\right)^{\delta_i}
      \left(\left(1 - F\left(u_i\right)\right)g\left(u_i\right)\right)^{1 - \delta_i}
    \right\}    \\
    =& \prod\limits_{i = 1}^n\left\{
      f\left(u_i\right)^{\delta_i}\left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
    \right\}
    \cdot \prod\limits_{i = 1}^n\left\{
      g\left(u_i\right)^{1 - \delta_i}\left(1 - G\left(u_i\right)\right)^{\delta_i}
    \right\}
  \end{aligned}
\end{equation}
So the core to maximize $L\left(F, G\right)$ with respect to $F$ in \eqref{eq:likelihood_full} is the same as that in \eqref{eq:likelihood_core}.

\subsection{Parametric MLE}
\label{sec:parametric-mle}

\subsubsection{One-sample setting}
\label{sec:one-sample-setting}

Suppose $T_1, \cdots, T_n$ are i.i.d. $Exp\left(\lambda\right)$, and subject to noninformative right censoring. Then \eqref{eq:likelihood_core} becomes

\[
  L = L\left(\lambda\right) =
  \prod\limits_{i = 1}^m\left\{
    \left(
      \lambda e^{-\lambda u_i}
    \right)^{\delta_i}
    \left(
      e^{-\lambda u_i}
    \right)^{1 - \delta_i}
  \right\}
  = \lambda^{\sum\limits_{i = 1}^n \delta_i}
  e^{-\lambda \sum\limits_{i = 1}^n u_i}
  = \lambda^r
  e^{-\lambda W}
  ,
\]
where $r = \sum\limits_{i = 1}^n \delta_i$ is the number of observed events and $W = \sum\limits_{i = 1}^n u_i$ is the total of observed time. Therefore $\mathrm{log}L = r\mathrm{log}\lambda - \lambda W$ and the MLE for $\lambda$ is
\[
  \hat{\lambda} = \frac{r}{W}
  .
\]
Furthermore, we know that
\[
  \left\{
    \begin{aligned}
      \frac{\partial \mathrm{log}L}{\partial \lambda} &= \frac{r}{\lambda} - W    \\
      \frac{\partial^2 \mathrm{log}L}{\partial \lambda^2} &= - \frac{r}{\lambda^2}
    \end{aligned}
  \right.
  .
\]
Based on properties of fisher information ({\color{blue} See the notes about fisher information for more details.}), we know that at the {\color{red}true underlying value} $\lambda$, it must satisfy
\begin{equation}
  \label{eq:fisher_info_property}
  \left\{
    \begin{aligned}
      \mathrm{E}\frac{\partial \mathrm{log}L}{\partial \lambda}
      &= \frac{\mathrm{E}r}{\lambda} - \mathrm{E}W
      &&= 0    \\
      I\left(\lambda\right)
      &= -\mathrm{E}\frac{\partial^2 \mathrm{log}L}{\partial \lambda^2}
      &&= \frac{\mathrm{E}r}{\lambda^2}    \\
      I^\star\left(\lambda\right)
      &= \frac{1}{n}I\left(\lambda\right)
      &&= \frac{\mathrm{E}r}{n\lambda^2}
    \end{aligned}
  \right.
  .    
\end{equation}
Note that in \eqref{eq:fisher_info_property}, $r$ and $W$ are random variables. And the probability to observe an event is
\[
  p = P\left(\delta_i = 1\right)
  = P\left(U_i \leq \infty, \delta_i = 1\right)
  = \int_0^\infty f\left(t\right)\left(1 - G\left(t\right)\right)\mathrm{d}t
  .
\]
Therefore $r\sim binomial\left(n, p\right)$, $\mathrm{E}r = np$. And from property of MLE, we can write
\[
  \frac{\sqrt{n}\left(\hat{\lambda} - \lambda\right)}{\sqrt{I^\star\left(\lambda\right)^{-1}}}
  = \frac{\left(\hat{\lambda} - \lambda\right)}{\sqrt{I\left(\lambda\right)^{-1}}}
  \overset{D}{\rightarrow}
  N\left(0, 1\right)
  ,
\]
which means approximately
\[
  \hat{\lambda}
  \overset{\mathrm{apx}}{\sim}
  N\left(\lambda, I\left(\lambda\right)^{-1}\right)
  = N\left(\lambda, \frac{\lambda^2}{np}\right)
  .
\]
Unfortunately, both $\lambda$ and $p$ (essentially $G\left(\cdot\right)$) are unknown. We plug in the estimation $\hat{\lambda} = r / W$ and $\hat{p} = r / W$ and apply Slutsky's theorem. This means for the purpose of estimation, we use
\begin{equation}
  \label{eq:fisher_info_estimate}
  \left\{
    \begin{aligned}
      & \hat{\lambda} = \frac{r}{W}    \\
      & \hat{I\left(\lambda\right)} = \frac{r}{\hat{\lambda}^2}
      ,\quad
      \hat{I^\star\left(\lambda\right)} = \frac{r}{n\hat{\lambda}^2}
    \end{aligned}
  \right.
\end{equation}
Not that unlike \eqref{eq:fisher_info_property}, here in \eqref{eq:fisher_info_estimate}, $r$ and $W$ are observations. And we have
\begin{equation}
  \label{eq:normal_approximation}
    \hat{\lambda} \overset{\mathrm{apx}}{\sim}
  N\left(\lambda, \frac{r}{W^2}\right)
  .
\end{equation}
Note that it turns out that a better approximation is to assume $\mathrm{log}\hat{\lambda}$ is normal. Using the delta method, this gives
\begin{equation}
  \label{eq:lognormal_approximation}
    \mathrm{log}\hat{\lambda}
  \overset{\mathrm{apx}}{\sim}
  N\left(
    \mathrm{log}\lambda,
    \frac{1}{np}
  \right)
  \approx
  N\left(\mathrm{log}\lambda, \frac{1}{r}\right)
  .
\end{equation}
\par
Now based on \eqref{eq:normal_approximation} or \eqref{eq:lognormal_approximation}, we can construct CI on $\lambda$, which also means we can perform hypothesis testing about $\lambda$.

\subsubsection{Two-sample setting}
\label{sec:two-sample-setting}

For two samples $x_1, \cdots, x_n$ and $y_1, \cdots, y_m$, both follow exponential distribution with parameters $\lambda_1$ and $\lambda_2$. Assume noninformative censoring in each group, using same tech in Section~\ref{sec:one-sample-setting} we can get
\[
  Z = \frac{
    \mathrm{log}\hat{\lambda_1} - \mathrm{log}\hat{\lambda_2}
  }{
    \sqrt{\frac{1}{r_1} + \frac{1}{r_2}}
  }
  \overset{\mathrm{apx}}{\sim}
  N\left(0, 1\right)
  .
\]

\subsection{Nonparametric MLE}
\label{sec:nonparametric-mle}

The NPMLE of survivor function $S\left(\cdot\right)$ based on i.i.d. survival time and non-informative right censoring is often known as Kaplan-Meier estimator or the Product-Limit Estimator. Here we provide some heuristic development, but formal proofs will be deferred to other notes. With the same notation as before, the observed data is
\[
  U_i = \mathrm{min}\left(T_i, C_i\right),\quad\quad \delta_i = 1\left(T_i \leq C_i\right),
\]
where $T_i$s are i.i.d survival times and $C_i$s are i.i.d {\color{red} non-informative} censoring time. The full likelihood is already shown in \eqref{eq:likelihood_full}.

\subsubsection{Discrete time points}
\label{sec:discrete-time-points}

To begin with, let's assume $F\left(\cdot\right)$ takes discrete values with mass points at $\left\{v_i\right\}$s: $0 \leq v_1 < v_2 < \cdots < \cdots$, and define the discrete hazard functions as
\begin{equation}
  \label{eq:def_discrete_hazard_function}
    \begin{aligned}
    & h_1 = P\left(T = v_1\right)    \\
    & h_j = P\left(T = v_j\middle| T > v_{j - 1}\right)
    \quad\quad j > 1.
  \end{aligned}
\end{equation}
Note that \eqref{eq:def_discrete_hazard_function} can be seen as discrete version of \eqref{eq:def_hazard_function}. And for $t \in {\color{red} \left[\right.} v_j, v_{j + 1}\left.\right)$,
\[
  \begin{aligned}
    S\left(t\right)
    &\overset{\mathrm{def}}{=} P\left(T {\color{red} >} t\right) = P\left(T {\color{red} > }v_j\right)    \\
    &= P\left(T > v_{j}\middle|T > v_{j - 1}\right)P\left(T > v_{j - 1}\right)    \\
    &= P\left(T > v_{j}\middle|T > v_{j - 1}\right)
    P\left(T > v_{j - 1}\middle| T > v_{j - 2}\right)P\left(T > v_{j - 2}\right)    \\
    & = \cdots    \\
    &= P\left(T > v_1\right)\prod\limits_{i = 1}^{j - 1}P\left(T > v_{i + 1}\middle| T > v_{i}\right)    \\
    &= \prod\limits_{i = 1}^j\left(1 - h_i\right)
    \quad\quad j > 1
    .
  \end{aligned}
\]
For discrete case, the p.m.f $f\left(\cdot\right)$ is
\[
  \begin{aligned}
    f\left(v_1\right) &= P\left(T = v_1\right) = h_1    \\
    f\left(v_j\right) &= P\left(T = v_j\right)
    = P\left(T = v_j\middle|T > v_{j - 1}\right)P\left(T > v_{j - 1}\right)
    = h_j\prod\limits_{i = 1}^{j - 1}\left(1 - h_i\right)
    .
  \end{aligned}
\]
Then if we want to estimate $F\left(\cdot\right)$ from likelihood, either \eqref{eq:likelihood_core} or \eqref{eq:likelihood_full}, we are just trying to maximizing
\[
  \begin{aligned}
    L\left(F\right)
    &= \prod\limits_{i = 1}^n\left\{
      f\left(u_i\right)^{\delta_i}
      \left(1 - F\left(u_i\right)\right)^{1 - \delta_i}
    \right\}    \\
    &= \prod\limits_{\left\{u_i\middle|\delta_i = 1\right\}}
    f\left(u_i\right)
    \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
    S\left(u_i\right)
    .
  \end{aligned}
\]
Let $I\left(\cdot\right)$ be an index mapping function that returns the index in $v_i$s that matches $u_i$, i.e. $I\left(u_i\right) = j$ if and only if $u_i\in\left[v_j, v_{j + 1}\right)$. Then we know that $u_i = v_{I\left(u_i\right)}$ and we can write
\begin{equation}
  \label{eq:discrete_likelihood}
    \begin{aligned}
    L\left(F\right)
    &= \prod\limits_{\left\{u_i\middle|\delta_i = 1\right\}}
      f\left(v_{I\left(u_i\right)}\right)
      \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
      S\left(v_{I\left(u_i\right)}\right)    \\
    &= \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i = v_1\right\}}
      f\left(v_1\right)
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i \neq v_1\right\}}
      f\left(v_{I\left(u_i\right)}\right)
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right)}\left(1 - h_k\right)
      \right]    \\
    &= \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i = v_1\right\}}
      h_1
      \right]
      \left[
            \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i \neq v_1\right\}}
      \left(
      h_{I\left(u_i\right)}
      \prod\limits_{k = 1}^{I\left(u_i\right) - 1}
      \left(1 - h_k\right)
      \right)
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right)}\left(1 - h_k\right)
      \right]    \\
    &= \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1\right\}}
      h_{I\left(u_i\right)}
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i \neq v_1\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right) - 1}
      \left(1 - h_k\right)
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right)}\left(1 - h_k\right)
      \right]
      .
  \end{aligned}
\end{equation}
Note that in \eqref{eq:discrete_likelihood}, the first part is
\begin{equation}
  \label{eq:discrete_likelihood_part1}
  \prod\limits_{\left\{u_i\middle|\delta_i = 1\right\}}
  h_{I\left(u_i\right)}
  = \prod\limits_{j = 1}^{\infty}
  h_j^{d_j}
  ,
\end{equation}
where $d_j = \sum\limits_{i = 1}^n\delta_i \cdot 1\left(u_i = v_j\right)$ is the number of event at $v_j$. The second and third part in \eqref{eq:discrete_likelihood} is
\begin{equation}
  \label{eq:discrete_likelihood_part23}
  \begin{aligned}
    & \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 1, u_i \neq v_1\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right) - 1}
      \left(1 - h_k\right)
      \right]
      \left[
      \prod\limits_{\left\{u_i\middle|\delta_i = 0\right\}}
      \prod\limits_{k = 1}^{I\left(u_i\right)}\left(1 - h_k\right)
      \right]    \\
    =& \left[
       \prod\limits_{k = 1}^{\infty}
       \prod\limits_{\left\{
       i\middle|\delta_i = 1, I\left(u_i\right) - 1 \geq k
       \right\}}
       \left(1 - h_k\right)
       \right]
       \left[
       \prod\limits_{k = 1}^{\infty}
       \prod\limits_{\left\{i|\delta_i = 0, I\left(u_i\right)\geq k\right\}}
       \left(1 - h_k\right)^{}
       \right]    \\
    =& \left[
       \prod\limits_{k = 1}^{\infty}
       \left(1 - h_k\right)^{\sum\limits_{i = 1}^n \delta_i \cdot 1\left(I\left(u_i\right) - 1 \geq k\right)}
       \right]
       \left[
       \prod\limits_{k = 1}^{\infty}
       \left(1 - h_k\right)^{\sum\limits_{i = 1}^n \left(1 - \delta_i\right) \cdot 1\left(I\left(u_i\right) \geq k\right)}
       \right]    \\
    =& \left[
       \prod\limits_{k = 1}^{\infty}
       \left(1 - h_k\right)^{
       \sum\limits_{i = 1}^n \delta_i \cdot \left[
       1\left(I\left(u_i\right) \geq k\right)
       - 1\left(I\left(u_i\right) = k\right)
       \right]
       }
       \right]
       \left[
       \prod\limits_{k = 1}^{\infty}
       \left(1 - h_k\right)^{\sum\limits_{i = 1}^n \left(1 - \delta_i\right) \cdot 1\left(I\left(u_i\right) \geq k\right)}
       \right]    \\
    =& \prod\limits_{k = 1}^{\infty}
       \left(1 - h_k\right)^{Y\left(v_k\right) - d_k}
       ,
  \end{aligned}
\end{equation}
where
\[
  Y\left(v_k\right) = \sum\limits_{i = 1}^n1\left(I\left(u_i\right)\geq k\right)
  = \sum\limits_{i = 1}^n1\left(u_i \geq v_k\right)
\]
is the number of subjects that are ``at risk'' at time $v_k$. \textbf{Note: }by the word ``at risk'', we also count the subjects that died just at $v_k$, which means $Y\left(v_j\right) \geq d_j$. But we do NOT count the subjects that are censored before $v_j$.
\par
Then from \eqref{eq:discrete_likelihood_part1} and \eqref{eq:discrete_likelihood_part23} we know that \eqref{eq:discrete_likelihood} can be written as
\begin{equation}
  \label{eq:discrete_likelihood_rewrite}
  L\left(F\right) = \prod\limits_{j = 1}^{\infty}h_j^{d_j}\left(1 - h_j\right)^{Y\left(v_j\right) - d_j}
  .
\end{equation}
And the NPMLE is just
\begin{equation}
  \label{eq:discrete_NPMLE_hazard}
  \hat{h}_j = \frac{d_j}{Y\left(v_j\right)}
\end{equation}
for $j = 1, \cdots, \infty$ and $Y\left(v_j\right) > 0$. \eqref{eq:discrete_NPMLE_hazard} implies some properties of this discrete NPMLE:
\begin{enumerate}
\item This estimation makes sense: the probability of dying at $v_j$ given the fact you live past $v_{j - 1}$ can be estimated by the proportion of subjects die at $v_j$ over the number of ``at risk'' at $v_j$.
\item $\hat{h}_j$ is only defined at time points where $Y\left(v_j\right) > 0$. Therefore, for large enough $v_j$, there will be no observation, no matter event or censoring, resulting inability to make estimation about hazard at those time points.
\item For time points where $Y\left(v_j\right) >0$ but no event occurs, the hazard is estimated to be 0.
\end{enumerate}
This means
\[
  \hat{S}\left(t\right) =\left\{
  \begin{aligned}    
    & 1 && t < v_1    \\
    & \prod\limits_{j = 1}^{k}\left(1 - \hat{h}_j\right) && v_{k} \leq t < v_{k + 1}
  \end{aligned}
  \right.
\]
\textbf{Note: } $S\left(\cdot\right)$ is defined to be {\color{red} right-continuous}.
\par
Let $v_g$ denotes the largest time point with observation, which means $Y\left(v_g\right) > 0$ and $Y\left(v_{g+ 1}\right) = 0$. Then either $d_g = Y\left(v_g\right)$ or $d_g < Y\left(v_g\right)$. If $d_g = Y\left(v_g\right)$, then $\hat{h}_g = 1$ and $\hat{S}\left(t\right) = 0$ for $t \geq v_g$. But if $d_g < Y\left(v_g\right)$, then $\hat{S}\left(t\right) > 0$ for $v_g \leq t < v_{g+1}$ and $\hat{S}\left(t\right)$ is undefined on $t\in\left[v_{g + 1}, \infty\right)$.
\par
Here one might say that the KM estimator is undefined on $t\in\left[v_{g+ 1}, \infty\right)$. Or another explanation is that NPMLE is not unique and any survival function that is identical to $\hat{S}$ at previous time is the NPMLE.

\subsubsection{Continuous time points}
\label{sec:cont-time-points}

Now, if we don't know in advance the times at which $F$ had mass, or even did not want to assume $F$ was discrete distribution? The core of likelihood still takes the form of \eqref{eq:likelihood_core}, but now we have to maximize it over all c.d.f., including discrete, continuous and mixed distributions.
\par
Kaplan and Meier argue that the solution must be a discrete distribution with mass on the observed times $u_i$ only. That is, the KM (product-limit) estimator of $F\left(\cdot\right)$ is
\begin{equation}
  \label{eq:km_estimator}
  \hat{S}\left(t\right) =\left\{
  \begin{aligned}    
    & 1 && t < v_1    \\
    & \prod\limits_{j = 1}^{k}
      \left(1 - \frac{d_j}{Y\left(v_j\right)}\right) && v_{k} \leq t < v_{k + 1}
  \end{aligned}
  \right.  
\end{equation}
Note that \eqref{eq:km_estimator} only puts weight at the observed ({\color{red} uncensored}) failure time. Another (equivalent) representation of $\hat{S}\left(t\right)$ is given by
\begin{equation}
  \label{eq:km_estimator_2}
  \hat{S}\left(t\right) = \prod\limits_{j:v_j \leq t}\left(
    \frac{Y\left(v_j\right) - d_j}{Y\left(v_j\right)}
  \right)
  \quad\quad \text{for } t \leq \mathrm{max}\left(v_i\right)
  ,
\end{equation}
where $v_1 < v_2 < \cdots$ are the distinct observed failure time.

\subsubsection{Some extensions}
\label{sec:some-extensions}

Besides the KM-estimator, there is also Efron's ``Redistribution of Mass'' algorithm that gives the same results.
\par
Another point of view is that the KM estmator can be seen as the self-consistency estimator. If there's no censoring, the survival function can be estimated as
\[
  \hat{S}\left(t\right) = n^{-1} \sum\limits_{i = 1}^n1\left(T_i > t\right)
  .
\]
In the precense of censoring, and the observed data $\left\{\left(U_i, \delta_i\right), i = 1, \cdots, n\right\}$, the survival function can be estimated as
\[
  \hat{S}\left(t\right)
  = n^{-1}\sum\limits_{i = 1}^n
  \mathrm{E}\left\{
    1\left(T_i > t\right)
    \middle| U_i, \delta_i
  \right\}
\]
where
\[
  \begin{aligned}
    \mathrm{E}\left\{1\left(T > t\right)\middle| U_i, \delta_i = 1\right\}
    =& 1(U_i > t)    \\
    \mathrm{E}\left\{1\left(T > t\right)\middle| U_i, \delta_i = 0\right\}
    =& \mathrm{E}\left(
       1\left(T > t, U_i \leq t\right)
       + 1\left(T > t, U_i > t\right)\middle| U_i, \delta_i = 0\right)    \\
    =& \mathrm{E}\left(
       1\left(T > t\right)\middle| U_i = u_i \leq t, \delta_i = 0\right)
       1\left(u_i \leq t\right)
       + 1\left(U_i > t\right)    \\
    =& \mathrm{E}\left(
       1\left(T > t\right)\middle| T > u_i\right)
       1\left(u_i \leq t\right)
       + 1\left(U_i > t\right)    \\
        =& P\left(
       T > t\middle| T > u_i\right)
       1\left(u_i \leq t\right)
       + 1\left(U_i > t\right)    \\
    =& S\left(t\right) / S\left(U_i\right) 1\left(t \geq U_i\right)
       + 1\left(t < U_i\right)
       .
  \end{aligned}
\]
Unfortunately, $S\left(\cdot\right)$ is unknown, and we calculate $\hat{S}\left(\cdot\right)$ iteratively via
\[
  \begin{aligned}
    \hat{S}_{new}\left(t\right)
    =& n^{-1}\sum\limits_{i = 1}^n\left\{
       \delta_i \cdot 1\left(U_i > t\right)
       + \left(1 - \delta_i\right) \cdot 1\left(U_i \leq t\right)
       \cdot \frac{\hat{S}_{old}\left(t\right)}{\hat{S}_{old}\left(U_i\right)}
       + \left(1 - \delta_i\right) \cdot 1\left(U_i > t\right)
       \right\}    \\
    =& n^{-1}\sum\limits_{i = 1}^n\left\{
       1\left(U_i > t\right)
       + \left(1 - \delta_i\right) \cdot 1\left(U_i \leq t\right)
       \cdot \frac{\hat{S}_{old}\left(t\right)}{\hat{S}_{old}\left(U_i\right)}
       \right\}
  \end{aligned}
\]
And the limit $\hat{S}\left(t\right)$ solves
\[
  \hat{S}\left(t\right)
  = n^{-1}\sum\limits_{i = 1}^n\left\{
    1\left(U_i > t\right)
    + \left(1 - \delta_i\right) \cdot 1\left(U_i \leq t\right)
    \cdot \frac{\hat{S}\left(t\right)}{\hat{S}\left(U_i\right)}
  \right\}
  ,
\]
which gives the same results as KM estimator.

\par

Since $H\left(t\right) = -\mathrm{log}\left(S\left(t\right)\right)$, it follows that a nonparametric estimator for $H\left(t\right)$ is
\[
  \tilde{H}\left(t\right) = -\mathrm{log}\left(\hat{S}\left(t\right)\right) =
  -\sum\limits_{i = 1}^k\mathrm{log}\left(1 - \hat{h}_i\right)
  \quad\quad\text{for } v_k\leq t < v_{k + 1}
  .
\]
Note that $\mathrm{log}\left(1 - x\right) \approx \mathrm{log}\left(1\right) + \frac{-1}{1 - x}|_{x = 0}\cdot x = -x$, therefore an alternative estimator (for $k\geq 1$) is
\[
  \hat{H}\left(t\right) = \sum\limits_{i = 1}^k\hat{h}_i
  \quad\quad\text{for } v_k\leq t < v_{k + 1}
  .  
\]
And this is called the {\color{red}Nelson-Aalen} estimator of $H\left(\cdot\right)$.
\par
Now what if we want to approximate the distribution of $\hat{S}\left(t\right)$? One can use the large-sample property of MLE (but the ordinary regularity conditions do not hold here since it is not a finite-dimensional parameter space). Nevertheless, let's proceed as if this is not a problem.

\bibliographystyle{plainnat}
\bibliography{../ref}





\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
