\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
% (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
% \typeout{(#1)}
% \@addtofilelist{#1}
% \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
%   \makeatother
%   \newcommand*{\myexternaldocument}[1]{%
%   \externaldocument{#1}%
%   \addFileDependency{#1.tex}%
%   \addFileDependency{#1.aux}%
% }
%   \myexternaldocument{external_tex_file}
%   --- end of overleaf setting ---

%   mathtools can be used to define labeling format for equations
%   one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
% you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
% you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{Cox Proportional Hazard Model}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle
\tableofcontents{}

\section{Introduction}
\label{sec:introduction}

In this note we will talk about the Cox's proportional hazards (Cox's PH) model. Suppose we observe some non-informativly right-censored data $\left(U, \delta\right)$ with covariate vector $Z$. That is, for subject $i$, the covariate vector is $Z_i$, survival time $T_i$ and censoring time $C_i$. The observed data is $\left(U_i, \delta_i\right)$ where $U_i = \mathrm{min}\left(T_i, C_i\right)$ and $\delta_i = 1\left(T_i \leq C_i\right)$. Also $T_i \perp C_i | Z_i$.
\par
And now we want to model the relationship between $Z$ and $T$. One way to do that is to incorporate $Z$ into the hazard function $h\left(\cdot\right)$, e.g.,
\[
  T \sim Exp\left(\lambda_Z\right)
  \implies
  h\left(t\right) = \lambda_Z
  \overset{\Delta}{=} e^{\alpha + \beta Z} = \lambda_0 e^{\beta Z}
  ,
\]
where $\lambda_0 = e^{\alpha}$ can be viewed as a baseline hazard. If $\beta = 0$ then $Z$ is not associated with $T$.
\par
We can generalize this idea as
\[
  h\left(t\middle|Z\right)
  = h_0\left(t\right) \times g\left(Z\right)
  .
\]
So the hazard can be factorized and this model is sometimes called a ``multiplicative intensive model'' or ``multiplicative hazard model'' or ``proportional hazard model'' because this factorization implies that
\[
  \frac{
    h\left(t\middle| Z = z_1\right)}{
    h\left(t\middle| Z = z_2\right)
  }
  = \frac{g\left(z_1\right)}{g\left(z_2\right)}
  .
\]
The hazard ratio is constant with respect to $t$, hence the (constant) proportional hazard. So in our previous model (the exponential survival time), the hazard ratio is
\[
  \frac{h\left(t\middle| Z = z_1\right)}{h\left(t\middle|Z = z_2\right)}
  = e^{\beta\left(z_1 - z_2\right)}
  .
\]
Also this exponential form of $g\left(Z\right)$
\begin{equation}
  \label{eq:cox_ph_form}
  h\left(t\middle|Z\right) = h_0\left(t\right)
  \cdot e^{\beta Z}  
\end{equation}
is the \textbf{Cox's PH} model.

\section{Estimation}
\label{sec:estimation}

In this section, we will talk about what is the objective function for Cox's model. But we will not talk about the detailed optimization algorithm. \eqref{eq:cox_ph_form} implies that
\[
  \begin{aligned}
    S\left(t\middle|Z\right)
    =& \mathrm{exp}\left(-H\left(t\middle|Z\right)\right)    \\
    =& \mathrm{exp}\left(-\int_0^th\left(u\middle|Z\right)\mathrm{d}u\right)    \\
    =& \mathrm{exp}\left(-\int_0^th_0\left(t\right)\mathrm{d}u \cdot g\left(Z\right)\right)    \\
    =& \left(S_0\left(t\right)\right)^{g\left(Z\right)}
    = \left(S_0\left(t\right)\right)^{\mathrm{exp}\left(\beta Z\right)}
    ,
  \end{aligned}
\]
where $S_0\left(t\right) = \mathrm{exp}\left( - \int_0^t h_0\left(u\right)\mathrm{d}u\right)$, the survival function for $Z = 0$, hence $S\left(t\middle|Z = 0\right)$. Also remember that $f\left(t\middle|Z\right) = h\left(t\middle|Z\right) S\left(t\middle|Z\right)$. Thus, given $n$ independent data $\left(u_i, \delta_i, z_i\right)$, the likelihood ({\color{blue} one can refer to our previous notes about survival analysis.}) is
\begin{equation}
  \label{eq:general_full_likelihood}
    \begin{aligned}
    L\left(\beta, h_0\left(\cdot\right)\right)
    =& \prod\limits_{i = 1}^n
    \left(
      f\left(u_i\middle|z_i\right)
    \right)^{\delta_i}
    \left(
      S\left(u_i\middle|z_i\right)
    \right)^{1 - \delta_i}
    = \prod\limits_{i = 1}^n
    h\left(u_i\middle|z_i\right)^{\delta_i}
    S\left(u_i\middle|z_i\right)    \\
    =& \prod\limits_{i = 1}^n
    \left(
      h_0\left(u_i\right)
      e^{\beta z_i}
    \right)^{\delta_i}
    \left(
      \mathrm{exp}\left(-\int_0^{u_i}h_0\left(t\right)\mathrm{d}t\right)
    \right)^{\mathrm{exp}\left(\beta z_i\right)}    \\
    =& \text{function $\left(data, h_0\left(\cdot\right), \beta\right)$}.
  \end{aligned}
\end{equation}
If $h_0\left(\cdot\right)$ is allowed to be ``arbitary'', then the ``parameter space `` is
\[
  \mathcal{H} \times \mathcal{R}^p
  = \left\{
    \left(h\left(\cdot\right), \beta\right)
    \middle|
    h_0\left(\cdot\right) \geq 0,
    \int_0^\infty h_0\left(t\right)\mathrm{d}t = \infty,
    \beta \in \mathcal{R}^p
  \right\}
  ,
\]
where $\int_0^\infty h_0\left(t\right)\mathrm{d}t = \infty$ ensures that $S_0\left(\infty\right) = 0$.
\par
In general this likelihood is hard to maximize. And Cox proposed this idea: to factor $L\left(\beta, h_0\left(\cdot\right)\right)$ as
\[
  L\left(\beta, h_0\left(\cdot\right)\right)
  = L_1\left(\beta\right)
  \times
  L_2\left(\beta, h_0\left(\cdot\right)\right)
  ,
\]
where $L_1$ only depends on $\beta$ and its maximization ($\hat{\beta}$) enjoys nice properties such as consistency and asymptotic normality while $L_2$ contains relatively little information about $\beta$. And this $L_1$ is called a \textbf{partial likelihood}.

\subsection{What is $L_1\left(\beta\right)$}
\label{sec:what-is}

In this section we introduce the $L_1$ proposed by Cox. First let's assume there are \textbf{NO tied} nor censoring observations. And define the distinct times of failure $\tau_1 < \tau_2 < \cdots$. Denote
\[
  R_j = \left\{i\middle|U_i{\color{red} \geq}\tau_j\right\}
  = \text{risk set at }\tau_j
  ,
\]
and
\[
  Z_{\left(j\right)} = \text{value of Z for the subject who fails at $\tau_j$}
  .
\]
we can reconstruct the data from $\left\{\tau_j\right\}$, $\left\{R_j\right\}$ and $\left\{Z_{\left(j\right)}\right\}$. And $L_1$ is defined as
\begin{equation}
  \label{eq:partial_likelihood}
  L_1\left(\beta\right)
  \overset{\Delta}{=}
  \prod\limits_{j}\left\{
    \frac{e^{\beta Z_{{\color{red}\left(j\right)}}}}{
      \sum\limits_{l\in R_{j}}e^{\beta Z_{{\color{red}l}}}
    }
  \right\}
  .
\end{equation}

{\color{red}(Cox model assumes the time measurement to be continuous, but here we think about discrete time point for some intuition.)} 

\subsubsection{Intuition: profile likelihood perspective}
\label{sec:prof-likel-persp}
Note that under this setting (no tie, no censor), the full likelihood \eqref{eq:general_full_likelihood} becomes
\[
  L\left(\beta, h_0\left(\cdot\right)\right)
  = \prod\limits_{i = 1}^n
  h_0\left(u_i\right)
  e^{\beta z_i}
  \left(
    \mathrm{exp}\left(-\int_0^{u_i}h_0\left(t\right)\mathrm{d}t\right)
  \right)^{\mathrm{exp}\left(\beta z_i\right)}
  .
\]
Furthermore, we can assume $u_i = \tau_i$, i.e. the data has been \underline{sorted} based on survival time. And use the KM idea, i.e. assume the survival function is \textbf{discrete} with \underline{baseline} hazard value $h_j$ at $u_j$. Then this likelihood becomes
\begin{equation}
  \label{eq:discrete_likelihood}
    L\left(\beta, h_1, \cdots, h_n\right)
  = \prod\limits_{i = 1}^n h_i e^{\beta z_i}
  \mathrm{exp}\left(
    - \sum\limits_{j = 1}^i h_j
  \right)^{\mathrm{exp}\left(\beta z_i\right)}
  .
\end{equation}
Note that, in previous notes we have deduct that in discrete case, for any $t\in {\color{red}\left[\right.} v_j, v_{j + 1})$:
\[
  H\left(t\right) = \sum\limits_{i = 1}^j h_i
  \quad\quad
  S\left(t\right) = \prod\limits_{i = 1}^j\left(1 - h_i\right)
  .
\]
Here in \eqref{eq:discrete_likelihood} we use the approximation that $e^{-h_j} \approx 1 - h_j$ when $h_j$ is close to 0.
\par
We can use the method of \underline{profile likelihood}: That is, for any given $\beta$, we maximize $L$ (or equivalently, $\mathrm{log}L$) over $h_j$s so the result is a function of $\beta$. Taking derivative, we have
\[
  \frac{\partial \mathrm{log}L}{\partial h_j}
  = \frac{1}{h_j} - \sum\limits_{i \leq j}\mathrm{exp}\left(\beta z_i\right)
  ,\quad\quad
  j = 1, \cdots, n.
\]
Set them to 0 we have $\hat{h}_j = 1 / \sum\limits_{i \leq j} \mathrm{exp}\left(\beta z_i\right)$. And the \underline{log} profile likelihood of $\beta$ is
\begin{equation}
  \label{eq:profile_loglikelihood}
  \begin{aligned}
    \mathrm{log}L_{profile}\left(\beta\right) =&
    \mathrm{log}\left\{
      \prod\limits_{i = 1}^n
      \frac{\mathrm{exp}\left(\beta z_i\right)}{
        \sum\limits_{k \leq i}\mathrm{exp}\left(\beta z_k\right)
      }
      \mathrm{exp}\left(
        - \sum\limits_{j = 1}^i
        \frac{1}{\sum\limits_{k \leq j}\mathrm{exp}\left(\beta z_k\right)}
      \right)^{\mathrm{exp}\left(\beta z_i\right)}
    \right\}    \\
    =& \sum\limits_{i = 1}^n\left\{
      \mathrm{log}\left\{
        \frac{\mathrm{exp}\left(\beta z_i\right)}{
          \sum\limits_{k \leq i}\mathrm{exp}\left(\beta z_k\right)
        }
        \mathrm{exp}\left(
          - \sum\limits_{j = 1}^i
          \frac{1}{\sum\limits_{k \leq j}\mathrm{exp}\left(\beta z_k\right)}
        \right)^{\mathrm{exp}\left(\beta z_i\right)}
      \right\}
    \right\}    \\
    =& \sum\limits_{i = 1}^n\left\{
      \mathrm{log}\left(
        \frac{\mathrm{exp}\left(\beta z_i\right)}{
          \sum\limits_{k \leq i}\mathrm{exp}\left(\beta z_k\right)
        }
      \right)
      - \mathrm{exp}\left(\beta z_i\right)
      \cdot
      \left(
        \sum\limits_{j = 1}^i
        \frac{1}{\sum\limits_{k \leq j}\mathrm{exp}\left(\beta z_k\right)}
      \right)
    \right\}    \\
    =& \sum\limits_{i = 1}^n\left\{
      \mathrm{log}\left(
        \frac{\mathrm{exp}\left(\beta z_i\right)}{
          \sum\limits_{k \leq i}\mathrm{exp}\left(\beta z_k\right)
        }
      \right)\right\}
      - \sum\limits_{i = 1}^n
        \sum\limits_{j = 1}^i
        \frac{\mathrm{exp}\left(\beta z_i\right)}{\sum\limits_{k \leq j}\mathrm{exp}\left(\beta z_k\right)}
        ,
  \end{aligned}
\end{equation}
where the second part of last equation can be reduced to {\color{red}$-n$?}, which means
\[
  L_{profile}\left(\beta\right)
  \propto \prod\limits_{i = 1}^n
  \frac{\mathrm{exp}\left(\beta z_i\right)}{
    \sum\limits_{k \leq i}\mathrm{exp}\left(\beta z_k\right)
  }
  .
\]
And this is what Cox uses as $L_1\left(\beta\right)$.

\subsubsection{Intuition: conditional distribution perspective}
\label{sec:cond-distr-persp}

Given the fact that someone survies up to just prior to $\tau_j$, hence in the risk set $R_j$, the hazard of someone with covariate value $z$ failing at $t = \tau_j$ is
\[
  h_0\left(\tau_j\right)\mathrm{exp}\left(\beta z\right)
  .
\]
In discrete case, this is the conditional probability {\color{blue}(in continuous case, this hazard value can go beyond 1.)} of someone fails at $\tau_j$ given the fact that subject survives past $\tau_{j - 1}$.
\par
Now, given the risk set $R_j$ and the fact that the subject with $z_\star$ fails at $\tau_j$, this conditional so-called ``probability''/``likelihood'' is
\[
  \frac{h_0\left(\tau_j\right)\mathrm{exp}\left(\beta z_\star\right)}{
    \sum\limits_{l\in R_j}h_0\left(\tau_j\right)\mathrm{exp}\left(\beta z_l\right)
  }
  = \frac{
    \mathrm{exp}\left(\beta z_\star\right)
  }{
    \sum\limits_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
  }
  .
\]
Then the cumproduct over all $\tau_j$ leads to $L_1$ in Cox's model.
\par
\textbf{Note:} strictly speaking, this is not a conditional \underline{probability}. In \underline{discrete} case, let $h\left(z_1, \tau\right), \cdots, h\left(z_n, \tau\right)$ denotes the hazard of subject $i$ at $\tau$, which is the conditional probability of the subject fails at $\tau_j$ given the fact that the subject survives past $\tau_{j - 1}$. Then given the risk set $R_j$ and the fact that exactly one subject fails at $\tau_j$. The probability of that subject being $z_i, i\in R_j$ is
\[
  \frac{h\left(z_i, \tau\right)
    \prod\limits_{j\neq i}\left(1 - h\left(z_j, \tau\right)\right)}{
    \sum\limits_{i \in R_j}\left(
      h\left(z_i, \tau\right)
      \prod\limits_{l\in R_j, l\neq i}\left(1 - h\left(z_l, \tau\right)\right)
    \right)
  }
\]


\subsection{What if there is censoring?}
\label{sec:what-if-theres}

Then \eqref{eq:partial_likelihood} is still used.

\subsection{What if there are tied event times?}
\label{sec:what-if-there}

Through the probability of tie existance is 0 in the continuous time case, in real life it is pretty comman. Let
\[
  \begin{aligned}
    & \tau_1 < \tau_2 < \cdots < \tau_k
    &&\quad \text{distinct failure times}    \\
    & d_j
    &&\quad \text{number of failures at $\tau_j$}    \\
    & z_{\left(j\right)}^{\left(1\right)}, z_{\left(j\right)}^{\left(2\right)}, \cdots, z_{\left(j\right)}^{\left(d_j\right)}
    &&\quad \text{values of z for the $d_j$ subjects who fail at $\tau_j$}    \\
    & R_j &&\quad \text{as before}
  \end{aligned}
\]
Then the exact, and two approximation of the partial likelihood are shown below.

\subsubsection{Exact partial likelihood}
\label{sec:exact-part-likel}

The exact partial likelihood considers all the possible rankings for the tied observations. Specifically
\begin{equation}
  \label{eq:exact_l1_with_ties}
  \footnotesize
  \begin{aligned}
    L_1\left(\beta\right)
    =& \prod\limits_{j = 1}^K
    \left\{
      \sum\limits_{
        \left(k_1, \cdots, k_{d_j}\right) = \left(1, 2, \cdots, d_j\right)
      }
      \prod\limits_{i = 1}^{d_j}\left\{
        \frac{
          \mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(k_i\right)}\right)
        }{
          \sum_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
          - \sum\limits_{s = 1}^{i - 1}\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(k_s\right)}\right)
        }
      \right\}
    \right\}    \\
    =& \prod\limits_{j = 1}^{K}\left\{
      \left[
        \frac{
         \prod\limits_{i = 1}^{d_j}\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(i\right)}\right) 
       }{
         \sum\limits_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
       }        
     \right]
     \cdot
     \left[
       \sum\limits_{\left(k_1, \cdots, k_{d_j}\right) = \left(1, 2, \cdots, d_j\right)}
       \prod\limits_{{\color{red}i = 2}}^{d_j}\frac{1}{
         \sum\limits_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
         - \sum\limits_{s = 1}^{i - 1}
         \mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(k_s\right)}\right)
       }
     \right]
    \right\}
    .
  \end{aligned}
\end{equation}
The computation of the exact partial likelihood gets out of hand pretty quickly as $d_j$ increases. So some modification/approximation methods are proposed.

\subsubsection{Breslow's approximation}
\label{sec:bresl-appr}

\begin{equation}
  \label{eq:breslow_l1}
  L_1\left(\beta\right) =
  \prod\limits_{j = 1}^K\left\{
    \prod\limits_{i = 1}^{d_j}\left\{
      \frac{\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(i\right)}\right)}{
        \sum\limits_{l\in R_j}\mathrm{exp}\left(\beta z_{l}\right)
      }
    \right\}
  \right\}
  = \prod\limits_{j = 1}^K\left\{
    \frac{\prod\limits_{i = 1}^{d_j}\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(i\right)}\right)}{
      \left(
        \sum\limits_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
      \right)^{d_j}
    }
  \right\}
  .
\end{equation}
The idea is to treat these $d_j$ subjects separately as that in \eqref{eq:partial_likelihood}, the \textbf{same} risk set is used for each and product the results together.

\subsubsection{Efron's approximation}
\label{sec:efrons-approximation}

\begin{equation}
  \label{eq:efron_l1}
  L_1\left(\beta\right) =
  \prod\limits_{j = 1}^K\left\{
    \frac{
      \prod\limits_{i = 1}^{d_j}\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(i\right)}\right)
    }{
      \prod\limits_{i = 1}^{d_j}\left\{
        \sum_{l\in R_j}\mathrm{exp}\left(\beta z_l\right)
        - \frac{i - 1}{d_j}
        \sum\limits_{s = 1}^{d_j}\mathrm{exp}\left(\beta z_{\left(j\right)}^{\left(s\right)}\right)
      \right\}
    }
  \right\}
  ,
\end{equation}
which is quite accurate for moderate $d_j$.

\subsubsection{One example for comparison}
\label{sec:one-example-comp}

Let's assume $R_j = \left\{1, 2, 3\right\}$ and death set at $\tau_j$ is $D_j = \left\{1, 2\right\}$, which means \underline{two} tied event at $\tau_j$. Then at this time point,
\begin{enumerate}
\item the exact partial likelihood, from \eqref{eq:exact_l1_with_ties} is
\[
  \begin{aligned}
    & \frac{
      e^{\beta z_1}
    }{
      e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
    }
    \times \frac{
      e^{\beta z_2}
    }{
      e^{\beta z_2} + e^{\beta z_3}
    }
    + \frac{
      e^{\beta z_2}
    }{
      e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
    }
    \times \frac{
      e^{\beta z_1}
    }{
      e^{\beta z_1} + e^{\beta z_3}
    }    \\
    =& \frac{
      e^{\beta\left(z_1 + z_2\right)}
    }{
      e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
    }
    \times \left(
      \frac{1}{e^{\beta z_2} + e^{\beta z_3}}
      + \frac{1}{e^{\beta z_1} + e^{\beta z_3}}
    \right)
  \end{aligned}
\]
\item the breslow'a approximation, from \eqref{eq:breslow_l1} is
  \[
    \begin{aligned}
      & \frac{
      e^{\beta\left(z_1 + z_2\right)}
    }{
      e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
    }
      \times
      \frac{1}{
        e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
      }    \\
      =& \frac{
        e^{\beta\left(z_1 + z_2\right)}
      }{
        \left(
          e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
        \right)^2
      }
    \end{aligned}
  \]
\item the efron's approximation, from \eqref{eq:efron_l1} is
  \[
    \frac{
      e^{\beta\left(z_1 + z_2\right)}
    }{
      e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
      }
      \times
      \frac{1}{
        e^{\beta z_1} + e^{\beta z_2} + e^{\beta z_3}
        - \frac{1}{2}\left(
          e^{\beta z_1} + e^{\beta z_2}
        \right)
      }
  \]
\end{enumerate}
So normally speaking, efron's approximation is more accurate than breslow's, but breslow's is more easire to cmpute. In real application, when $d_j$ is big, it may be appropriate to consider analysis for discrete survival function.


\section{Inference}
\label{sec:inference}

The idea is to just proceed with partial likelihood as if it is the full likelihood.
\begin{itemize}
\item Maximize $L_1$ over $\beta$. The maximization of full likelihood is sometimes called the ``semiparametric'' MLE. Usually the estimate $\hat{\beta} = \mathrm{argmax}L_1$ can not be obtained in closed form.
\item Approximate the variance of $\hat{\beta}$ by inverse of observed information from $L_1$.
\item Use Wald test, score test, LRT as in ordinary ML settings.
\end{itemize}


Here we consider a scalar value $z$. And we can see how it is easy to compute using the breslow's approximation. It is easy to verify from \eqref{eq:breslow_l1} that
\[
  \begin{aligned}
    & U\left(\beta\right) =
  \frac{\partial \mathrm{log}L_1\left(\beta\right)}{\partial \beta}
  = \sum\limits_{j = 1}^K\left\{
    \sum\limits_{i = 1}^{d_j}z_{\left(j\right)}^{\left(i\right)}
    - d_j
    \cdot
    \sum\limits_{l \in R_j}w_l^{\left(j\right)}z_l
  \right\}    \\
  & \hat{I}\left(\beta\right)
  = -\frac{\partial^2\mathrm{log}L_1\left(\beta\right)}{\partial \beta^2}
  = {\color{red}\text{to be added.}}
  ,
  \end{aligned}
\]
where
\[
  w_l^{\left(j\right)} = \frac{
    e^{\beta z_l}
  }{
    \sum\limits_{m\in R_j}e^{\beta z_m}
  }
  .
\]

\paragraph{Wald test:}

based on
\[
  \hat{\beta} \overset{apx}{\sim}
  N\left(\beta, \hat{I}^{-1}\left(\hat{\beta}\right)\right)
  .
\]

\paragraph{Score test:}

The null hypothesis is $H_0:\beta = 0$, based on this assuming $U\left(0\right) / \sqrt{I\left(0\right)} \overset{apx}{\sim}N\left(0, 1\right)$.



\bibliographystyle{plainnat}
\bibliography{../ref}





\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
