\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
% (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
% \typeout{(#1)}
% \@addtofilelist{#1}
% \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
%   \makeatother
%   \newcommand*{\myexternaldocument}[1]{%
%   \externaldocument{#1}%
%   \addFileDependency{#1.tex}%
%   \addFileDependency{#1.aux}%
% }
%   \myexternaldocument{external_tex_file}
%   --- end of overleaf setting ---

%   mathtools can be used to define labeling format for equations
%   one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
% you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
% you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{Test for the probability of a binomial distribution}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle

For an i.i.d sample from a bernoulli distribution
\[
  x_1, \cdots, x_n \overset{\mathrm{i.i.d.}}{\sim} Bernoulli(p)
  ,
\]
The likelihood of the data is
\[
  f\left(x_1, \cdots, x_n\right)
  = \prod\limits_{i = 1}^np^{x_i}\left(1 - p\right)^{1 - x_i}
  = p^{\sum x_i}\left(1 - p\right)^{n - \sum x_i}
  .
\]
MLE for $p$ is $\bar{x} = \frac{1}{n}\sum x_i$ and
\[
  \sum\limits_{i = 1}^nx_i \sim Binom(n, p)
  .
\]

So here are mainly two situations: One is to test the probability $p$ against some given value $p_0$. The other is to compare the probability between two independent random samples $x_1, \cdots, x_n$ and $y_1, \cdots, y_m$.
\begin{enumerate}[label = Case\arabic*:]
\item One sample $x_1, \cdots, x_n$ from $Bernoulli(p)$, and test $p$ against a given $p_0$.
\item Two samples: $x_1, \cdots, x_2$ from $Bernoulli(p_1)$ and $y_1, \cdots, y_m$ from $Bernoulli(p_2)$. And test whether $p_1 = p_2$.
\end{enumerate}

\section{Normal approximation}
\label{sec:normal-approximation}

\subsection{Case 1}
\label{sec:case-1}

Note that
\[
  \mathrm{E}X = p,\quad\mathrm{Var}X = p\left(1 - p\right).
\]
Then by CLT we have
\[
  \bar{x} \overset{\mathrm{asymp}}{\sim} N\left(p,\;\frac{p\left(1 - p\right)}{n}\right)
  .
\]
For $H_0:\;p = p_0$, we propose a test statistic
\[
  Z = \frac{\bar{x} - p_0}{\sqrt{\frac{p_0\left(1 - p_0\right)}{n}}}
  .
\]
Then $Z$ is asymptotically standard normal under $H_0$.
\par
Also we know that under $H_1$:
\[
  \begin{aligned}
    Z =& \frac{\bar{x} - p_0}{\sqrt{\frac{p_0\left(1 - p_0\right)}{n}}}    \\
    =& \frac{\bar{x} - p}{\sqrt{\frac{p\left(1 - p\right)}{n}}}
    \cdot \sqrt{\frac{p\left(1 - p\right)}{p_0\left(1 - p_0\right)}}
    + \frac{p - p_0}{\sqrt{\frac{p_0\left(1 - p_0\right)}{n}}}    \\
    \sim& N\left(
      \frac{p - p_0}{\sqrt{\frac{p_0\left(1 - p_0\right)}{n}}}
      ,\quad
      \frac{p\left(1 - p\right)}{p_0\left(1 - p_0\right)}
    \right)
    .
  \end{aligned}
\]
So the power of the test can be easily computed.

\subsection{Case 2}
\label{sec:case-2}

So we have
\[
  \bar{x} \overset{\mathrm{asymp}}{\sim} N\left(p_1,\;\frac{p_1\left(1 - p_1\right)}{n}\right)
  ,\quad\text{and}\quad
  \bar{y} \overset{\mathrm{asymp}}{\sim} N\left(p_2,\;\frac{p_2\left(1 - p_2\right)}{m}\right)
  .
\]
A test statistic can be
\[
  Z = \frac{\bar{x} - \bar{y}}{
    \sqrt{
      \hat{p}\left(1 - \hat{p}\right)
      \left(\frac{1}{n} + \frac{1}{m}\right)
    }
  }
  ,
\]
where $\hat{p} = \frac{n\bar{x} + m\bar{y}}{n + m}$. This test statistic can be found at
\begin{verbatim}
https://stats.stackexchange.com/questions/361015/
  proof-of-the-standard-error-of-the-distribution-between-two-normal-distributions/
  361048#361048

https://stats.stackexchange.com/questions/113602/
  test-if-two-binomial-distributions-are-statistically-different-from-each-other
\end{verbatim}
Here this $\hat{p}\left(1 - \hat{p}\right)$ can be seen as an estimate for the variance $p\left(1 - p\right)$ when $H_0$ is true by directly plugging in $\hat{p}$. This is \textbf{NOT} a pooled variance for these two samples, which should always be no greater than $\hat{p}\left(1 - \hat{p}\right)$.
\par
The power of this test statistic is hard to compute under $H_1$.
\par
\textbf{Note: } One can also use the same idea in the ``t-test.pdf'' notes and propose the test statistic
\[
  T = \frac{\bar{x} - p_0}{\sqrt{S_x / n}}
  ,
\]
where $S_x = \frac{1}{n - 1}\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)$ for Case 1.
\par
And for Case 2
\[
  T = \frac{\bar{x} - \bar{y} - \Delta}{\sqrt{
      \left(\frac{1}{n} + \frac{1}{m}\right)S_p
    }}
  ,
\]
where $S_p^2 = \frac{\left(n - 1\right)S_x^2 + \left(m - 1\right)S_y^2}{n + m - 2}$ and $\Delta = p_1 - p_2$. But again it is hard to evaluate the testing power of these statistics.


\section{Chi-square approximation}
\label{sec:chi-square-appr}

See the notes of ``chisq\_test.pdf'' for details.


\section{Exact test}
\label{sec:exact-test}

\subsection{Case 1: Clopper-Pearson test}
\label{sec:case-1:-clopper}

The Clopper-Pearson method is an early method. It's called exact method because it's directly based on p.m.f of binomial distribution. Let $X = \sum\limits_{i = 1}^nx_i$. Then $X\sim Binom(n, p)$ and the p.m.f is
\begin{equation}
  \label{eq:binomial_pmf}
  f\left(x;p\right) = P\left(X = x\middle|p\right) = C_n^xp^x\left(1 - p\right)^{n - x}
  ,
\end{equation}
for $x = 0, 1, \cdots, n$. One thing to point out is that though the $|p$ notation, \eqref{eq:binomial_pmf} is frequenist's point of view, not bayesian's. Now let's recall that p-value is the probability under $H_0$ that something \textbf{as or more extreme than} what we have observed happens. Then after observing $X = x_0$, for one-sided test:
\begin{itemize}
\item $H_0: p \leq p_0$ against $H_1: p > p_0$ for some given $p_0$. The p-value at this observed $x_0$ is
  \begin{equation}
    \label{eq:clopper_pval_h0_leq}
    p_{val}\left(x_0\right) = \sum\limits_{x = x_0}^nf\left(x;p_0\right)
    .
  \end{equation}

\item $H_0: p \geq p_0$ against $H_1: p < p_0$ for some given $p_0$. The p-value at this observed $x_0$  is
  \begin{equation}
    \label{eq:clopper_pval_h0_geq}
    p_{val}\left(x_0\right) = \sum\limits_{x = 0}^{x_0}f\left(x;p_0\right)
    .    
  \end{equation}

\end{itemize}
For the two-sided test. This is a little complicated. Let index set
\[
  \mathcal{I} = \left\{
    x\middle|
    P\left(X = x\middle|p_0\right) \leq P\left(X = x_0\middle|p_0\right)
    ,\quad
    0\leq x \leq n
  \right\}
  .
\]
Then $\mathcal{I}$ contains all possible realizations of $X$ with its probability no greater than the probability of our observation. Then the p-value of $H_0: p = p_0$  at this observed $x_0$ is given by
\begin{equation}
  \label{eq:clopper_pval_h0_twosided}
  p_{val}\left(x_0\right) = \sum\limits_{x\in\mathcal{I}}f\left(x;p_0\right)
  . 
\end{equation}

\subsubsection{Power analysis}
\label{sec:power-analysis}

The probability to reject $H_0$ of Clopper-Pearson test at given underlying $p$ can be computed by
\begin{equation}
  \label{eq:clopper_power}
  P\left(\text{Reject } H_0\middle| p\right)
  = \sum\limits_{x = 0}^nP\left(X = x\middle|p\right)\cdot I_{\{p_{val}\left(x\right)\leq \alpha\}}
  = \sum\limits_{x = 0}^nf\left(x;p\right)\cdot I_{\{p_{val}\left(x\right)\leq \alpha\}}
  ,
\end{equation}
where $\alpha$ is the significant level of the test and $p_{val}\left(x\right)$ is computed for different types of $H_0$ based on \eqref{eq:clopper_pval_h0_leq}, \eqref{eq:clopper_pval_h0_geq} and \eqref{eq:clopper_pval_h0_twosided}.

\subsubsection{Confidence interval}
\label{sec:confidence-interval}

First for the one-sided intervals:

\begin{itemize}
\item $\left(P_L, 1\right]$: From \eqref{eq:clopper_pval_h0_leq}, $H_0: p \leq p_0$ is rejected when probability of observing $x_0$ or more number of success at $p_0$ is small enough. Therefore the reject area
  \[
    \text{Rejct Area: }
    \left\{
      x_0:\;\sum\limits_{x = x_0}^nf\left(x;p_0\right)\leq\alpha
    \right\}
    .
  \]
  Hence the accept area
  \[
    \text{Accept Area: }
    \left\{
      x_0:\;\sum\limits_{x = x_0}^nf\left(x;p_0\right) > \alpha
    \right\}
  \]
  Then we can construct the one-sided CI by increasing $p_0$ from 0 such that the first $p_0$ that satisfies this Accept area rule. Then that is the $P_L$. Therefore
  \begin{equation}
    \label{eq:one_sided_pl}
    \sum\limits_{x = x_0}^nf\left(x;P_L\right) = \alpha
    .    
  \end{equation}

\item $\left[0, P_U\right)$: Similar idea, from \eqref{eq:clopper_pval_h0_geq} we can construct the accept area
  \[
    \text{Accept Area: }
    \left\{
      x_0;\sum\limits_{x=0}^{x_0}f\left(x;p_0\right)>\alpha
    \right\}
    .
  \]
  Therefore we decrease $p_0$ from 1 to find the first $P_U$ that satisfies this Accept area rule. Therefore
  \begin{equation}
    \label{eq:one_sided_pu}
    \sum\limits_{x = 0}^{x_0}f\left(x;P_U\right) = \alpha
    .    
  \end{equation}
\end{itemize}

Now for the two-sided intervals $\left(P_L, P_U\right)$: we apply the \textbf{equal-tail rule} and find $P_L$ and $P_U$ such that
\begin{equation}
  \label{eq:two_sided_p}
  \begin{aligned}
    & \sum\limits_{x = x_0}^nf\left(x;P_L\right) = \alpha / 2    \\
    & \sum\limits_{x = 0}^{x_0}f\left(x;P_U\right) = \alpha / 2
    .
  \end{aligned}
\end{equation}

This interval can also be expressed as
\[
  S_{\leq} \cap S_{\geq},
\]
or equivalently
\[
  \left(
    \mathrm{inf}S_{\geq}, \mathrm{sup}S_{\leq}
  \right)
  ,
\]
where
\[
  \begin{aligned}
    S_{\leq}\overset{\Delta}{=}\left\{
      \theta\middle|
      P\left(
        Binomial\left(n, \theta\right) \leq x
      \right)
      > \frac{\alpha}{2}
    \right\}    \\
    S_{\geq} \overset{\Delta}{=}\left\{
      \theta \middle|
      P\left(
        Binomial\left(n, \theta\right) \geq x
      \right)
      > \frac{\alpha}{2}
    \right\}
    .
  \end{aligned}
\]
One can utilize the relationship between the Binomial cummulative distribution function and \textbf{regularized incomplete beta function}, i.e. for $k = 0, \cdots, n$
\[
  \begin{aligned}
    &P\left(X\leq k\right) = \sum\limits_{i = 0}^{k}
    C_n^ip^i\left(1 - p\right)^{n - i}    \\
    =& \frac{\Gamma\left(n + 1\right)}{\Gamma\left(n - k\right)\Gamma\left(k + 1\right)}
    \int_0^{1 - p}t^{n - k - 1}\left(1 - t\right)^k\mathrm{d}t
    = pBeta\left(1 - p; n - k, k + 1\right)    \\
    =& \frac{\Gamma\left(n + 1\right)}{\Gamma\left(n - k\right)\Gamma\left(k + 1\right)}
    \int_{p}^1t^k\left(1 - t\right)^{n - k - 1}\mathrm{d}t
    = 1 - pBeta\left(p; k + 1, n - k\right)
    ,
  \end{aligned}
\]
where $pBeta\left(x;\alpha, \beta\right)$ represents the cummulative probability of $Beta\left(\alpha, \beta\right)$ distribution, cummulated from 0 to $x$. And it satisfies
\[
  pBeta\left(x; \alpha, \beta\right) = 1 - pBeta\left(1 - x; \beta, \alpha\right)
  ,\quad
  \forall x\in\left[0, 1\right].
\]
Similarly, for the quantile function $qBeta\left(p;\alpha, \beta\right)$, we can show that
\[
  1 - qBeta\left(p;\alpha, \beta\right) = qBeta\left(1 - p; \beta, \alpha\right)
  ,\quad \forall p\in\left[0, 1\right]
  .
\]
So we can see that $P_L$ and $P_U$ are actually satisfing
\[
  \begin{aligned}
    & 1 - \alpha / 2
    ={\color{red}(\geq)}
    P\left(X \leq x_0 - 1\middle| P_L\right)
    = pBeta\left(1 - P_L; n - x_0 + 1, x_0\right)\\
    \implies & 1 - P_L = {\color{red}(\leq)} qBeta\left(1 - \alpha / 2; n - x_0 + 1 ,x_0\right)    \\
    \implies & P_L ={\color{red}(\geq)} qBeta\left(\alpha / 2; x_0, n - x_0 + 1\right) 
  \end{aligned}
\]
For $P_L$, it's taking $\mathrm{inf}$, therefore
\begin{equation}
  \label{eq:qbeta_pl}
  P_L = qBeta\left(\alpha / 2; x_0, n - x_0 + 1\right)
  .
\end{equation}

Similarly, we have for $P_U$:
\[
  \begin{aligned}
    &\alpha / 2 = P\left(X \leq x_0\middle| P_U\right)
    = pBeta\left(1 - P_U;, n - x_0, x_0 + 1\right)    \\
    \implies & 1 - P_U = qBeta\left(\alpha / 2; n - x_0, x_0 + 1\right)    \\
    \implies & P_U = qBeta\left(1 - \alpha / 2; x_0 + 1, n - x_0\right)
  \end{aligned}
\]
Therefore
\begin{equation}
  \label{eq:qbeta_pu}
  P_U = qBeta\left(1 - \alpha / 2; x_0 + 1, n - x_0\right)
  .
\end{equation}

Also, note that this cumulative probability is also related to F-distribution via
\[
  P\left(X\leq x_0\right) = F\left(
    x = \frac{1 - p}{p}\frac{x_0 + 1}{n - x_0};
    d_1 = 2\left(n - x_0\right),
    d_2 = 2\left(x_0 + 1\right)
  \right)
\]
where $F\left(x;d_1, d_2\right)$ is the cummulative probability function of a F-distribution with degree of freedom $d_1$ and $d_2$, cummulated from 0 to $x$. The we have
\begin{equation}
  \label{eq:qf_p}
  \begin{aligned}
    & P_L = \left(
      1 + \frac{n - x_0 + 1}{
        x_0 \times qF\left(\frac{\alpha}{2}; 2x_0, 2\left(n - x_0 + 1\right)\right)
      }
    \right)^{-1}    \\
    & P_U = \left(
      1 + \frac{n - x_0}{
        \left(x_0 + 1\right) \times
        qF\left(1 - \frac{\alpha}{2}; 2\left(x_0 + 1\right), 2\left(n - x_0\right)\right)
      }
    \right)^{-1}
  \end{aligned}
\end{equation}
where $qF\left(\alpha; d_1, d_2\right)$ is the quantile function of F-distribution.


\subsection{Case 2: Fisher's exact test}
\label{sec:case-2:-fishers}

Fisher's exact test is a method for testing proportion difference. A toy example of a $2\times 2$ contingency table is shown in Table~\ref{tab:data_sample}. When the margin of this table is fixed($n_x$, $n_y$, $n_1$ and $n_2$), the probability for observing this table follows the hyper-genometric distribution
\[
  P\left(
    \#\{\text{Sample 1, Success}\} = a
  \right)
  \frac{
    C_{n_x}^aC_{n_y}^{n_1 - a}
  }{
    C_n^{n_1}
  }
  .
\]

\begin{table}[htbp]
  \centering
  \begin{tabular}{c|c|c|c}
    \hline
    & Success & Failure & Total    \\
    \hline
    Sample 1 & a & b & $n_x = a + b$    \\
    \hline
    Sample 2 & c & d & $n_y = c + d$     \\
    \hline
    Total & $n_1 = a + c$ & $n_0 = b + d$ & $n = a + b + c + d$    \\
    \hline                          
  \end{tabular}
  \caption{Data sample}
  \label{tab:data_sample}
\end{table}

We can compute the p-value based on the same idea from Section~\ref{sec:case-1:-clopper}. Note that here the p-value is a \textbf{conditional} one since it is conditional on the fixed marginal values. To simplify the notation, denote $X$ the number in cell (Sample 1, Success) and
\[
  f\left(x; n_x, n_y, n_1\right)
  = P\left(
    X = x\middle| n_x, n_y, n_1
  \right)
  = \frac{C_{n_x}^xC_{n_y}^{n_1 - x}}{C_{n_x + n_y}^{n_1}}
\]
Then for a observation with $X = x_0$ and fixed $n_x, n_y, n_1$:
\begin{itemize}
\item $H_0: p_x \geq p_y$ against $H_1: p_x < p_y$. The p-value can be computed as
  \begin{equation}
    \label{eq:fisher_one_sided_geq}
    p_{val} = \sum\limits_{i = 0}^{x_0}f\left(i;n_x, n_y, n_1\right)
    .
  \end{equation}
\item $H_0: p_x \leq p_y$ against $H_1: p_x > p_y$. The p-value can be computed as
  \begin{equation}
    \label{eq:fisher_one_sided_leq}
    p_{val} = \sum\limits_{i = x_0}^{n_1}f\left(i; n_x, n_y, n_1\right)
    .
  \end{equation}
\item $H_0: p_x = p_y$ against $H_1: p_x\neq p_y$. The p-value can be computed as
  \begin{equation}
    \label{eq:fisher_two_sided}
    p_{val} = \sum\limits_{i = a_L}^{a_U}
    f\left(i;n_x, n_y, n_1\right)
    I_{\left\{
        f\left(i;n_x, n_y, n_1\right)
        \leq f\left(x_0;n_x, n_y, n_1\right){\color{red}\delta}
      \right\}}
    ,
  \end{equation}
  where the summation limits $a_L = \mathrm{max}\left(0, n_1 - n_y\right)$ and $a_U = \mathrm{min}\left(n_x, n_1\right)$. Note that in an ideal world the {\color{red} red $\delta$} is just 1 in \eqref{eq:fisher_two_sided}. But in actuality, since the computation involves large factorials, especially when sample size is large, the numerical results might be inaccurate. To ensure a convervative test, $\delta$ is set to $1.0000001$ in R\citep{Helwig2020p-}.
\end{itemize}


\bibliographystyle{plainnat}
\bibliography{../ref}


\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
