\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
%   (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother
% \newcommand*{\myexternaldocument}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }
% \myexternaldocument{external_tex_file}
% --- end of overleaf setting ---

% mathtools can be used to define labeling format for equations
% one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
                               % you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
                            % you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{T-test}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle

\section{Basic knowledge}
\label{sec:basic-knowledge}

$\phi\left(x\right)$ and $\Phi\left(x\right)$ are pdf and cdf of standard normal distribution, respectively. We use $Z$ to represent a random variable that follows standard normal distribution and $z_{\alpha}$ the lower $\alpha$ quantile of standard normal distribution. Therefore
\[
  P\left(Z \leq z_{\alpha}\right) = \Phi\left(z_{\alpha}\right) = \alpha
  .
\]

\begin{thm}
  \label{thm:sample_thm}
  Let $x_1, \cdots, x_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2 <\infty$. Then
  \begin{enumerate}
  \item $\mathrm{E}\bar{x} = \mu$.
  \item $\mathrm{Var}\bar{x} = \sigma^2 / n$.
  \item $\mathrm{E}S^2 = \sigma^2$, where $S^2 = \frac{1}{n - 1}\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{thm:normal_thm}
  Let $x_1, \cdots, x_n$ be a random sample from $N\left(\mu, \sigma^2\right)$. Then
  \begin{enumerate}
  \item $\bar{X} \sim N\left(\mu, \sigma^2 / n\right)$.
  \item $\bar{X}$ is independent of $S^2$.
  \item $\left(n - 1\right)S^2 / \sigma^2$ follows a chi-squared distribution with $n - 1$ degree of freedom.
  \end{enumerate}
\end{thm}



\section{One-sample test}
\label{sec:one-sample}

Consider a random sample $x_1, \cdots, x_n$ from $N\left(\mu, \sigma^2\right)$. The likelihood is
\[
  \begin{aligned}
    f\left(x_1, \cdots, x_n\right)
    =& \prod\limits_{i = 1}^n
       \left(2\pi\sigma^2\right)^{-1/2}
       \mathrm{exp}\left(
       -\frac{
       \left(x_i - \mu\right)^2
       }{2\sigma^2}
       \right)    \\
    =& \left(2\pi\sigma^2\right)^{-n / 2}
       \mathrm{exp}\left(
       -\frac{\sum\limits_{i = 1}^n\left(x_i - \mu\right)^2}{2\sigma^2}
       \right)
       .
  \end{aligned}
\]
We propose the test
\[
  H_0:\; \mu = \mu_0
  \textbf{\quad v.s\quad}
  H_1:\; \mu \neq \mu_0
\]
\subsection{variance known}
\label{sec:variance-known}

Construct LRT
\[
  LR = \frac{
    \underset{\mu\in H_0}{\mathrm{max}}\;
    f\left(x_1, \cdots, x_n\middle|\mu\right)
  }{
    \underset{\mu\in H_0\cup H_1}{\mathrm{max}}\;
    f\left(x_1, \cdots, x_n\middle|\mu\right)
  }
  =\frac{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_0\right)
  }{
    f\left(x_1, \cdots, x_n\middle|\mu = \bar{x}\right)
  }
  = \mathrm{exp}\left(
    -\frac{\left(\bar{x} - \mu_0\right)^2}{2\sigma^2 / n}
  \right)
\]

Therefore rejecting $H_0$ when LR is smaller than some constant $C$ is equivalent to rejecting $H_0$ when $\left|\bar{x} - \mu_0\right|$ is larger than some other constant $C$. Hence
\[
  \text{Reject Region: }
  \left\{
    \bar{x}
    :\quad
    \left|\bar{x}-\mu_0\right| > C
  \right\}
\]

\subsubsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from}

From definition of $\alpha$ we know that $C$ in the reject region is chosen such that
\[
  P\left(
    \left|\bar{x} - \mu_0\right| > C
    \middle| H_0\text{ is true }\right)
  \leq \alpha
  .
\]
But to fully utilize the test, we choose to use equal sign instead of $\leq$. Therefore
\[
  P\left(
    \left|\bar{x} - \mu_0\right| > C
    \middle| \mu = \mu_0
  \right)
  = \alpha
  .
\]
Note that $\bar{x}\sim N\left(\mu, \sigma^2 / n\right)$. Then under the condition $\mu = \mu_0$,
\[
  \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  \sim N\left(0, 1\right)
  .
\]
Therefore we propose the reject region for $H_0$ being
\[
  \left|
    \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  \right|
  \geq z_{1 - \alpha / 2}
  .
\]
\textbf{Note: } Here, even if the sample distribution is not normal, the result still holds due to CLT under large sample.

\subsubsection{Power at given underlying $\mu$}
\label{sec:power-at-given}

The power (the probability to reject $H_0$, when $H_1$ is true) of the proposed test procedure for any given underlying $\mu \neq \mu_0$ is computed as
\begin{equation}
  \label{eq:power_equation_one_sample_sigma_known}
  \begin{aligned}
    & P\left(
      \left|
      \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
      \right|
      \geq z_{1 - \alpha / 2}
      \right)    \\
    = & P\left(
        \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
        \leq z_{\alpha / 2}
        \right)
        + P\left(
        \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
        \geq z_{1 - \alpha / 2}
        \right)    \\
    = & P\left(
        \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
        \leq z_{\alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)
        + P\left(
        \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
        \geq z_{1 - \alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)    \\
    = & P\left(
        Z
        \leq z_{\alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)
        + P\left(
        Z
        \geq z_{1 - \alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)    \\
  \end{aligned}  
\end{equation}

Here we use the fact that $  \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}} \sim N\left(0, 1\right)$. 

\subsubsection{Sample size at given $\alpha$, $\beta$ and underlying $\mu$}
\label{sec:sample-size-at}

W.l.o.g, assume that $\mu > \mu_0$, then in previous power equation \eqref{eq:power_equation_one_sample_sigma_known}
\[
  P\left(
    Z
    \leq z_{\alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right)
\]
would be really close to zero and
\[
  P\left(
    Z
    \geq z_{1 - \alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right) 
\]
will offer most of the power. In order to guarantee a power of at least $1 - \beta$, we could simply set
\[
    P\left(
    Z
    \geq z_{1 - \alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right) \geq 1 - \beta
  ,
\]
which means
\[
  z_{1 - \alpha / 2} + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}} \leq z_{\beta}
  .
\]
Normally in test settings, $\alpha < 0.1$ and $\beta < 0.5$, which means $z_{1 - \alpha / 2}$ is positive and $z_\beta$ is negative. Also $\mu_0 - \mu < 0$ in our assumption. This leads to
\[
  - z_{\alpha / 2} - z_{\beta} \leq \frac{\sqrt{n}\left(\mu - \mu_0\right)}{\sigma}
  .
\]
Hence the sample size requirement is
\begin{equation}
  \label{eq:samplesize_one_sample_sigma_known}
  n \geq \frac{\sigma^2\left(z_{\alpha / 2} + z_{\beta}\right)^2}{\left(\mu - \mu_0\right)^2}
  .  
\end{equation}
\textbf{Note:} The sample size requirement can be deduced the same way when $\mu < \mu_0$. And the result is just the same as \eqref{eq:samplesize_one_sample_sigma_known}.

\subsection{variance unknown}
\label{sec:variance-unknown}

When $\sigma^2$ is unknown, the MLE under $H_0$ is
\[
  \mu_{\left(0\right)} = \mu_0
  ,\quad
  \sigma^2_{\left(0\right)} = \frac{1}{n}\sum\limits_{i = 1}^n\left(x_i - \mu_0\right)^2
  .
\]
And the MLE under $H_0\cup H_1$ is
\[
  \mu_{\left(0 \cup 1\right)} = \bar{x}
  ,\quad
  \sigma^2_{\left(0 \cup 1\right)} = \frac{1}{n}\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
  .
\]
\textbf{Note: } MLE for $\sigma^2$ offers smaller MSE than $S^2$, but it's biased.
\par
Then the likelihood ratio is
\[
  LR = \frac{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_{\left(0\right)}, \sigma^2 = \sigma^2_{\left(0\right)}\right)
  }{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_{\left(0\cup 1\right)}, \sigma^2 = \sigma^2_{\left(0\cup 1\right)}\right)
  }
  = \left(
    \frac{
      \sum\limits_{i = 1}^n\left(x_i - \mu_0\right)^2
    }{
      \sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
    }
  \right)^{-n / 2}
  \propto \left(
    \frac{
      \sum\limits_{i = 1}^n\left(\bar{x} - \mu_0\right)^2
    }{
      \sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
    }
  \right)^{-n / 2}  
  ,
\]
where for the last part we mainly focus on terms related to $\mu_0$. So to reject $H_0$ when LR is small is equivalent to
\[
  \text{Reject Region: }
  \left\{
    \bar{x}
    :\quad
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2}} > C
  \right\}
\]

The idea is similar to that in Section~\ref{sec:variance-known}. But we replace $\sigma^2$ with $S^2$.

\subsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from-1}

First we can write
\[
  P\left(
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2}} > C
    \middle| \mu = \mu_0
  \right)
  = P\left(
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\left(n - 1\right)S^2}} > C
    \middle| \mu = \mu_0
  \right)
  = \alpha
  .  
\]

From Theorem~\ref{thm:normal_thm} we know that
\[
  \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}} \sim N\left(0, 1\right)
  ,\quad
  \left(n - 1\right)S^2 / \sigma^2 \sim \chi^2\left(n - 1\right)
  ,\quad
  \bar{x} \perp S^2
\]
Therefore
\[
  \frac{
    \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  = \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  \sim t(n - 1)
  .
\]
Then we know the reject rejion is
\[
  \left|
    \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  \right|
  > t_{1 - \alpha / 2}\left(n - 1\right)
  .
\]

\textbf{Note: } Here we need Theorem~\ref{thm:normal_thm}, which means the normal assumption of the sample is \textbf{necessary}. Though one might argue that without normal assumption, under large sample scenario, using Slutsky's theorem, asymptotically
\[
  \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  = \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
  \sqrt{\frac{\sigma^2}{S^2}}
  \rightarrow N\left(0, 1\right)
  .
\]

\subsection{Power at given underlying $\mu$ and $\sigma^2$}
\label{sec:power-at-given-1}

Before any computation, we introduce the \textbf{non-central} t-distribution.
\begin{equation}
  \label{eq:noncentral-t-definition}
  T = \frac{Z + \mu}{\sqrt{V / v}}
  ,  
\end{equation}
where $Z$ follows standard normal and $V$ follows $\chi^2\left(v\right)$ and $Z\perp V$. Then $T$ follows a non-central t-distribution with degree of freedom $v$ and non-central parameter $\mu$, denoted by $t\left(v, \mu\right)$.
\par
Then we know that
\[
  \frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}
  = \frac{
    \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  = \frac{
    \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
    + \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  \sim t\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  ,
\]
which means $\frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}$ follows a non-central t-distribution $t\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)$. Therefore the power can be computed as
\begin{equation}
  \label{eq:power_equation_one_sample_sigma_unknown}
  \begin{aligned}
    & P\left(
      \left|
      \frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}
      \right|
      \geq t_{1 - \alpha / 2}\left(n - 1\right)
      \right)    \\
    =& P\left(
       \left|
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \right|
       \geq t_{1 - \alpha / 2}\left(n - 1\right)
       \right)    \\
    =& P\left(
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \leq t_{\alpha / 2}\left(n - 1\right)
       \right)
       + P\left(
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \geq t_{1 - \alpha / 2}\left(n - 1\right)
       \right)
       .
  \end{aligned}  
\end{equation}

\subsection{Sample size at given $\alpha$, $\beta$ and underlying $\mu$ and $\sigma^2$}
\label{sec:sample-size-at-1}

W.l.o.g, assume $\mu > \mu_0$, then in the previous power equation~\eqref{eq:power_equation_one_sample_sigma_unknown}
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \leq t_{\alpha / 2}\left(n - 1\right)
  \right)
\]
would be close to zero and
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \geq t_{1 - \alpha / 2}\left(n - 1\right)
  \right)
\]
will offer the most power. In order to guarantee a power of at least $1 - \beta$, we could simply set
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \geq t_{1 - \alpha / 2}\left(n - 1\right)
  \right)
  \geq 1 - \beta
  , 
\]
which means
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq t_{\beta}\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  .
\]
There's no close form for this inequality, we should use some numerical method to solve for $n$.
\par
\textbf{Note: } If $\mu < \mu_0$, then similarly we can get the requirement as
\[
  t_{\alpha / 2}\left(n - 1\right) \geq t_{1 - \beta}\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  .
\]
Use the fact that $t_{\alpha}\left(n, \mu\right) = -t_{1 - \alpha}\left(n, -\mu\right)$, we can arrange the previous inequality as
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq
  t_{\beta}\left(n - 1, \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}\right)
  .
\]
Therefore in summary the sample size requirement is
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq
  t_{\beta}\left(n - 1, \frac{\left|\mu_0 - \mu\right|}{\sqrt{\sigma^2 / n}}\right)
  .
\]

\section{Two sample test}
\label{sec:two-sample-test}

Consider two random samples $x_1, \cdots, x_{n_1} \sim N\left(\mu_1, \sigma_1^2\right)$ and $y_1, \cdots, y_{n_2}\sim N\left(\mu_2, \sigma_2^2\right)$. Then the likelihood of the data is
\[
  \begin{aligned}
    & f\left(x_1, \cdots, x_{n_1}, y_1, \cdots, y_{n_2}
      \middle|\mu_1, \mu_2, \sigma_1^2, \sigma_2^2\right)    \\
    =& \left( 2\pi\sigma_1^2\right)^{-n_1 / 2}
       \left(2\pi\sigma_2^2\right)^{-n_2 / 2}
       \mathrm{exp}\left(
       - \frac{\sum\limits_{i = 1}^{n_1}\left(x_i - \mu_1\right)^2}{2\sigma_1^2}
       - \frac{\sum\limits_{i = 1}^{n_2}\left(y_i - \mu_2\right)^2}{2\sigma_2^2}
       \right)    \\
    =& 
  \end{aligned}
\]
We propose the test
\[
  H_0:\; \mu_1 = \mu_2
  \quad\textbf{v.s.}\quad
  H_1:\; \mu_1 \neq \mu_2
  .
\]

\subsection{Two-sample, variance known}
\label{sec:two-sample-variance}
When $\sigma_1^2$ and $\sigma_2^2$ are known, the likelihood satisfies
\[
 f\left(x_1, \cdots, x_{n_1}, y_1, \cdots, y_{n_2}
   \middle|\mu_1, \mu_2\right)
 \propto
 \mathrm{exp}\left(
   -\frac{n_1\left(\bar{x} - \mu_1\right)^2}{2\sigma_1^2}
   -\frac{n_2\left(\bar{y} - \mu_2\right)^2}{2\sigma_2^2}
 \right)
 .
\]
Therefore the LRT

\subsection{Two-sample, variance unknown but equal}
\label{sec:two-sample-variance-1}

\subsection{Two-sample, variance unknown and unequal}
\label{sec:two-sample-variance-2}







\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
