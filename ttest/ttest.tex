\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\renewcommand{\textfraction}{0.15}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.60}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{extarrows}
\usepackage{bm}
% \newcommand{\bm}{\symbfit}    % `bm` confilicts with `unicode-math`. In that case use \symbfit for bold math symbols
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
% \usepackage{enumerate}
\usepackage{enumitem}    % more flexible than `enumerate` package, the reference will carry the whole label appearance, not just the counter, unlike the `enumerate` package.
\usepackage{upgreek}    % 'upgreek' letters

% \pdfstringdefDisableCommands{\let\bm=\relax}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{assum}{Assumption}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}

\setcounter{topnumber}{5}    % Maximum number of floats that can appear at the top of a text page; default 2. 
\setcounter{bottomnumber}{5}   % Maximum number of floats that can appear at the bottom of a text page; default 1. 
\setcounter{totalnumber}{10}    % Maximum number of floats that can appear on a text page; default 3. 

\DeclareMathOperator*{\argmaxdown}{arg\,max}
\DeclareMathOperator*{\argmindown}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}

% cross-reference to other files
% the externaldocument should be compiled 
%   (and at least twice if you're using xr-hyper)
% \usepackage{xr-hyper}
% \usepackage{xr}

% --- external document (ordinary setting) ---
% \externaldocument{external_tex_file}
% --- end of ordinary setting ---

% --- external document (overleaf setting) ---
% externaldocument settings for Overleaf
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother
% \newcommand*{\myexternaldocument}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }
% \myexternaldocument{external_tex_file}
% --- end of overleaf setting ---

% mathtools can be used to define labeling format for equations
% one can use \eqref for a reference to a labeled equation.
\usepackage{mathtools}
\newtagform{supp}{(S-}{)}    % define a equation labeling format for suppliment
\usetagform{supp}            % use the supp format
\usetagform{default}         % use the default format

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% package for hyperlinks
% It's error-prone because hyper link is quite difficult
% due to the fact the typesetting environment is complex.
% So you can disable this package and finish the document.
% Then sort out the hyperlink thing.
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,CJKbookmarks=True]{hyperref}

% package for displaying highlighted codes
\usepackage{minted}

% package for input codec and output rendering font
\usepackage[utf8]{inputenc}    % it is always good practice to use utf8
                               % you can also try latin1, latin2, cp1252 and cp1250
\usepackage[T1]{fontenc}    % the default is T0, which only contains 128 characters
                            % you can try T1, T2A, T2B
% you can refer to https://www.overleaf.com/learn/latex/international_language_support#Font_encoding
% for more details.


\title{T-test}
\author{Chao Cheng}
\date{\today}



\begin{document}
\maketitle

\section{Basic knowledge}
\label{sec:basic-knowledge}

$\phi\left(x\right)$ and $\Phi\left(x\right)$ are pdf and cdf of standard normal distribution, respectively. We use $Z$ to represent a random variable that follows standard normal distribution and $z_{\alpha}$ the lower $\alpha$ quantile of standard normal distribution. Therefore
\[
  P\left(Z \leq z_{\alpha}\right) = \Phi\left(z_{\alpha}\right) = \alpha
  .
\]

\begin{thm}
  \label{thm:sample_thm}
  Let $x_1, \cdots, x_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2 <\infty$. Then
  \begin{enumerate}
  \item $\mathrm{E}\bar{x} = \mu$.
  \item $\mathrm{Var}\bar{x} = \sigma^2 / n$.
  \item $\mathrm{E}S^2 = \sigma^2$, where $S^2 = \frac{1}{n - 1}\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{thm:normal_thm}
  Let $x_1, \cdots, x_n$ be a random sample from $N\left(\mu, \sigma^2\right)$. Then
  \begin{enumerate}
  \item $\bar{X} \sim N\left(\mu, \sigma^2 / n\right)$.
  \item $\bar{X}$ is independent of $S^2$.
  \item $\left(n - 1\right)S^2 / \sigma^2$ follows a chi-squared distribution with $n - 1$ degree of freedom.
  \end{enumerate}
\end{thm}



\section{One-sample test}
\label{sec:one-sample}

Consider a random sample $x_1, \cdots, x_n$ from $N\left(\mu, \sigma^2\right)$. The likelihood is
\[
  \begin{aligned}
    f\left(x_1, \cdots, x_n\right)
    =& \prod\limits_{i = 1}^n
       \left(2\pi\sigma^2\right)^{-1/2}
       \mathrm{exp}\left(
       -\frac{
       \left(x_i - \mu\right)^2
       }{2\sigma^2}
       \right)    \\
    =& \left(2\pi\sigma^2\right)^{-n / 2}
       \mathrm{exp}\left(
       -\frac{\sum\limits_{i = 1}^n\left(x_i - \mu\right)^2}{2\sigma^2}
       \right)
       .
  \end{aligned}
\]
We propose the test
\[
  H_0:\; \mu = \mu_0
  \textbf{\quad v.s\quad}
  H_1:\; \mu \neq \mu_0
\]
\subsection{variance known}
\label{sec:variance-known}

Construct LRT
\[
  LR = \frac{
    \underset{\mu\in H_0}{\mathrm{max}}\;
    f\left(x_1, \cdots, x_n\middle|\mu\right)
  }{
    \underset{\mu\in H_0\cup H_1}{\mathrm{max}}\;
    f\left(x_1, \cdots, x_n\middle|\mu\right)
  }
  =\frac{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_0\right)
  }{
    f\left(x_1, \cdots, x_n\middle|\mu = \bar{x}\right)
  }
  = \mathrm{exp}\left(
    -\frac{\left(\bar{x} - \mu_0\right)^2}{2\sigma^2 / n}
  \right)
\]

Therefore rejecting $H_0$ when LR is smaller than some constant $C$ is equivalent to rejecting $H_0$ when $\left|\bar{x} - \mu_0\right|$ is larger than some other constant $C$. Hence
\[
  \text{Reject Region: }
  \left\{
    \bar{x}
    :\quad
    \left|\bar{x}-\mu_0\right| > C
  \right\}
\]

\subsubsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from}

From definition of $\alpha$ we know that $C$ in the reject region is chosen such that
\[
  P\left(
    \left|\bar{x} - \mu_0\right| > C
    \middle| H_0\text{ is true }\right)
  \leq \alpha
  .
\]
But to fully utilize the test, we choose to use equal sign instead of $\leq$. Therefore
\[
  P\left(
    \left|\bar{x} - \mu_0\right| > C
    \middle| \mu = \mu_0
  \right)
  = \alpha
  .
\]
Note that $\bar{x}\sim N\left(\mu, \sigma^2 / n\right)$. Then under the condition $\mu = \mu_0$,
\[
  \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  \sim N\left(0, 1\right)
  .
\]
Therefore we propose the reject region for $H_0$ being
\[
  \left|
    \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  \right|
  \geq z_{1 - \alpha / 2}
  .
\]
\textbf{Note: } Here, even if the sample distribution is not normal, the result still holds due to CLT under large sample.

\subsubsection{Power at given underlying $\mu$}
\label{sec:power-at-given}

The power (the probability to reject $H_0$, when $H_1$ is true) of the proposed test procedure for any given underlying $\mu \neq \mu_0$ is computed as
\begin{equation}
  \label{eq:power_equation_one_sample_sigma_known}
  \begin{aligned}
    & P\left(
      \left|
      \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
      \right|
      \geq z_{1 - \alpha / 2}
      \right)    \\
    = & P\left(
        \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
        \leq z_{\alpha / 2}
        \right)
        + P\left(
        \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
        \geq z_{1 - \alpha / 2}
        \right)    \\
    = & P\left(
        \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
        \leq z_{\alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)
        + P\left(
        \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
        \geq z_{1 - \alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)    \\
    = & P\left(
        Z
        \leq z_{\alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)
        + P\left(
        Z
        \geq z_{1 - \alpha / 2}
        + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
        \right)    \\
  \end{aligned}  
\end{equation}

Here we use the fact that $  \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}} \sim N\left(0, 1\right)$. 

\subsubsection{Sample size at given $\alpha$, $\beta$ and underlying $\mu$}
\label{sec:sample-size-at}

W.l.o.g, assume that $\mu > \mu_0$, then in previous power equation \eqref{eq:power_equation_one_sample_sigma_known}
\[
  P\left(
    Z
    \leq z_{\alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right)
\]
would be really close to zero and
\[
  P\left(
    Z
    \geq z_{1 - \alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right) 
\]
will offer most of the power. In order to guarantee a power of at least $1 - \beta$, we could simply set
\[
    P\left(
    Z
    \geq z_{1 - \alpha / 2}
    + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}
  \right) \geq 1 - \beta
  ,
\]
which means
\[
  z_{1 - \alpha / 2} + \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}} \leq z_{\beta}
  .
\]
Normally in test settings, $\alpha < 0.1$ and $\beta < 0.5$, which means $z_{1 - \alpha / 2}$ is positive and $z_\beta$ is negative. Also $\mu_0 - \mu < 0$ in our assumption. This leads to
\[
  - z_{\alpha / 2} - z_{\beta} \leq \frac{\sqrt{n}\left(\mu - \mu_0\right)}{\sigma}
  .
\]
Hence the sample size requirement is
\begin{equation}
  \label{eq:samplesize_one_sample_sigma_known}
  n \geq \frac{\sigma^2\left(z_{\alpha / 2} + z_{\beta}\right)^2}{\left(\mu - \mu_0\right)^2}
  .  
\end{equation}
\textbf{Note:} The sample size requirement can be deduced the same way when $\mu < \mu_0$. And the result is just the same as \eqref{eq:samplesize_one_sample_sigma_known}.

\subsection{variance unknown}
\label{sec:variance-unknown}

When $\sigma^2$ is unknown, the MLE under $H_0$ is
\[
  \mu_{\left(0\right)} = \mu_0
  ,\quad
  \sigma^2_{\left(0\right)} = \frac{1}{n}\sum\limits_{i = 1}^n\left(x_i - \mu_0\right)^2
  .
\]
And the MLE under $H_0\cup H_1$ is
\[
  \mu_{\left(0 \cup 1\right)} = \bar{x}
  ,\quad
  \sigma^2_{\left(0 \cup 1\right)} = \frac{1}{n}\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
  .
\]
\textbf{Note: } MLE for $\sigma^2$ offers smaller MSE than $S^2$, but it's biased.
\par
Then the likelihood ratio is
\[
  LR = \frac{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_{\left(0\right)}, \sigma^2 = \sigma^2_{\left(0\right)}\right)
  }{
    f\left(x_1, \cdots, x_n\middle|\mu = \mu_{\left(0\cup 1\right)}, \sigma^2 = \sigma^2_{\left(0\cup 1\right)}\right)
  }
  = \left(
    \frac{
      \sum\limits_{i = 1}^n\left(x_i - \mu_0\right)^2
    }{
      \sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
    }
  \right)^{-n / 2}
  \propto \left(
    \frac{
      \sum\limits_{i = 1}^n\left(\bar{x} - \mu_0\right)^2
    }{
      \sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2
    }
  \right)^{-n / 2}  
  ,
\]
where for the last part we mainly focus on terms related to $\mu_0$. So to reject $H_0$ when LR is small is equivalent to
\[
  \text{Reject Region: }
  \left\{
    \bar{x}
    :\quad
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2}} > C
  \right\}
\]

The idea is similar to that in Section~\ref{sec:variance-known}. But we replace $\sigma^2$ with $S^2$.

\subsubsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from-1}

First we can write
\[
  P\left(
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\sum\limits_{i = 1}^n\left(x_i - \bar{x}\right)^2}} > C
    \middle| \mu = \mu_0
  \right)
  = P\left(
    \frac{\left|\bar{x}-\mu_0\right|}{\sqrt{\left(n - 1\right)S^2}} > C
    \middle| \mu = \mu_0
  \right)
  = \alpha
  .  
\]

From Theorem~\ref{thm:normal_thm} we know that
\[
  \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}} \sim N\left(0, 1\right)
  ,\quad
  \left(n - 1\right)S^2 / \sigma^2 \sim \chi^2\left(n - 1\right)
  ,\quad
  \bar{x} \perp S^2
\]
Therefore
\[
  \frac{
    \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  = \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  \sim t(n - 1)
  .
\]
Then we know the reject rejion is
\[
  \left|
    \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  \right|
  > t_{1 - \alpha / 2}\left(n - 1\right)
  .
\]

\textbf{Note: } Here we need Theorem~\ref{thm:normal_thm}, which means the normal assumption of the sample is \textbf{necessary}. Though one might argue that without normal assumption, under large sample scenario, using Slutsky's theorem, asymptotically
\[
  \frac{\bar{x} - \mu}{\sqrt{S^2 / n}}
  = \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
  \sqrt{\frac{\sigma^2}{S^2}}
  \rightarrow N\left(0, 1\right)
  .
\]

\subsubsection{Power at given underlying $\mu$ and $\sigma^2$}
\label{sec:power-at-given-1}

Before any computation, we introduce the \textbf{non-central} t-distribution.
\begin{equation}
  \label{eq:noncentral-t-definition}
  T = \frac{Z + \mu}{\sqrt{V / v}}
  ,  
\end{equation}
where $Z$ follows standard normal and $V$ follows $\chi^2\left(v\right)$ and $Z\perp V$. Then $T$ follows a non-central t-distribution with degree of freedom $v$ and non-central parameter $\mu$, denoted by $t\left(v, \mu\right)$.
\par
Then we know that
\[
  \frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}
  = \frac{
    \frac{\bar{x} - \mu_0}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  = \frac{
    \frac{\bar{x} - \mu}{\sqrt{\sigma^2 / n}}
    + \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}
  }{
    \sqrt{
      \frac{\left(n - 1\right)S^2}{\left(n - 1\right)\sigma^2}
    }
  }
  \sim t\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  ,
\]
which means $\frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}$ follows a non-central t-distribution $t\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)$. Therefore the power can be computed as
\begin{equation}
  \label{eq:power_equation_one_sample_sigma_unknown}
  \begin{aligned}
    & P\left(
      \left|
      \frac{\bar{x} - \mu_0}{\sqrt{S^2 / n}}
      \right|
      \geq t_{1 - \alpha / 2}\left(n - 1\right)
      \right)    \\
    =& P\left(
       \left|
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \right|
       \geq t_{1 - \alpha / 2}\left(n - 1\right)
       \right)    \\
    =& P\left(
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \leq t_{\alpha / 2}\left(n - 1\right)
       \right)
       + P\left(
       T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
       \geq t_{1 - \alpha / 2}\left(n - 1\right)
       \right)
       .
  \end{aligned}  
\end{equation}

\subsubsection{Sample size at given $\alpha$, $\beta$ and underlying $\mu$ and $\sigma^2$}
\label{sec:sample-size-at-1}

W.l.o.g, assume $\mu > \mu_0$, then in the previous power equation~\eqref{eq:power_equation_one_sample_sigma_unknown}
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \leq t_{\alpha / 2}\left(n - 1\right)
  \right)
\]
would be close to zero and
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \geq t_{1 - \alpha / 2}\left(n - 1\right)
  \right)
\]
will offer the most power. In order to guarantee a power of at least $1 - \beta$, we could simply set
\[
  P\left(
    T\left(n - 1,  \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
    \geq t_{1 - \alpha / 2}\left(n - 1\right)
  \right)
  \geq 1 - \beta
  , 
\]
which means
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq t_{\beta}\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  .
\]
There's no close form for this inequality, we should use some numerical method to solve for $n$.
\par
\textbf{Note: } If $\mu < \mu_0$, then similarly we can get the requirement as
\[
  t_{\alpha / 2}\left(n - 1\right) \geq t_{1 - \beta}\left(n - 1, \frac{\mu - \mu_0}{\sqrt{\sigma^2 / n}}\right)
  .
\]
Use the fact that $t_{\alpha}\left(n, \mu\right) = -t_{1 - \alpha}\left(n, -\mu\right)$, we can arrange the previous inequality as
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq
  t_{\beta}\left(n - 1, \frac{\mu_0 - \mu}{\sqrt{\sigma^2 / n}}\right)
  .
\]
Therefore in summary the sample size requirement is
\[
  t_{1 - \alpha / 2}\left(n - 1\right)
  \leq
  t_{\beta}\left(n - 1, \frac{\left|\mu_0 - \mu\right|}{\sqrt{\sigma^2 / n}}\right)
  .
\]

\section{Two sample test}
\label{sec:two-sample-test}

Consider two random samples $x_1, \cdots, x_{n_1} \sim N\left(\mu_1, \sigma_1^2\right)$ and $y_1, \cdots, y_{n_2}\sim N\left(\mu_2, \sigma_2^2\right)$. Then the likelihood of the data is
\[
  \begin{aligned}
    & f\left(x_1, \cdots, x_{n_1}, y_1, \cdots, y_{n_2}
      \middle|\mu_1, \mu_2, \sigma_1^2, \sigma_2^2\right)    \\
    =& \left( 2\pi\sigma_1^2\right)^{-n_1 / 2}
       \left(2\pi\sigma_2^2\right)^{-n_2 / 2}
       \mathrm{exp}\left(
       - \frac{\sum\limits_{i = 1}^{n_1}\left(x_i - \mu_1\right)^2}{2\sigma_1^2}
       - \frac{\sum\limits_{i = 1}^{n_2}\left(y_i - \mu_2\right)^2}{2\sigma_2^2}
       \right)    \\
    =& 
  \end{aligned}
\]
We propose the test
\[
  H_0:\; \mu_1 = \mu_2
  \quad\textbf{v.s.}\quad
  H_1:\; \mu_1 \neq \mu_2
  .
\]

\subsection{Two-sample, variance known}
\label{sec:two-sample-variance}
When $\sigma_1^2$ and $\sigma_2^2$ are known, the likelihood satisfies
\[
 f\left(x_1, \cdots, x_{n_1}, y_1, \cdots, y_{n_2}
   \middle|\mu_1, \mu_2\right)
 \propto
 \mathrm{exp}\left(
   -\frac{n_1\left(\bar{x} - \mu_1\right)^2}{2\sigma_1^2}
   -\frac{n_2\left(\bar{y} - \mu_2\right)^2}{2\sigma_2^2}
 \right)
 .
\]

Therefore under $H_0$, the MLE for $\mu_1$ and $\mu_2$ is
\[
  \mu_{1\left(0\right)} = \mu_{2\left(0\right)} = \mu_{\left(0\right)}
  = \frac{
    \sigma_2^2n_1\bar{x} + \sigma_1^2n_2\bar{y}
  }{
    \sigma_2^2n_1 + \sigma_1^2n_2
  }
  .
\]
And under $H_0\cup H_1$, the MLE for $\mu_1$ and $\mu_2$ is
\[
  \mu_{1\left(0\cup1\right)} = \bar{x}
  ,\quad
  \mu_{2\left(0\cup1\right)} = \bar{y}
  .
\]
Then the likelihood ratio is 
\[
  \begin{aligned}
    LR =& \frac{
            \underset{H_0}{\mathrm{max}}\;
            f\left(\bm{x}, \bm{y}\middle|\mu_1, \mu_2\right)
          }{
            \underset{H_0\cup H_1}{\mathrm{max}}\;
            f\left(\bm{x}, \bm{y}\middle|\mu_1, \mu_2\right)
          }    \\
    =& \frac{
       f\left(\bm{x}, \bm{y}\middle|\mu_1 = \mu_2 = \mu_{\left(0\right)}\right)
       }{
       f\left(\bm{x}, \bm{y}\middle|\mu_1 = \mu_{1\left(0\cup1\right)}, \mu_2 = \mu_{2\left(0\cup1\right)}\right)
       }    \\
    \propto&
             \mathrm{exp}\left(
               -\frac{1}{2}\left(
               \frac{n_1\left(\bar{x} - \mu_{\left(0\right)}\right)^2}{\sigma_1^2}
               +\frac{n_2\left(\bar{y} - \mu_{\left(0\right)}\right)^2}{\sigma_2^2}
               \right)
             \right)    \\
    =& \mathrm{exp}\left(
       -\frac{1}{2}
       \frac{n_1n_2}{\sigma_2^2n_1 + \sigma_1^2 n_2}
       \left(\bar{x} - \bar{y}\right)^2
       \right)
       .
  \end{aligned}
\]

From the idea of LRT, $H_0$ is rejected when $LR$ is small enough, which means the reject rule is
\[
  \text{Reject Region: }
  \left\{
    \left(\bar{x}, \bar{y}\right)
    \middle|
    \left|\bar{x} - \bar{y}\right| > C
  \right\}
\]

\subsubsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from-2}

From the definition of $\alpha$ we know that 
\[
  P\left(
    \left|\bar{x} - \bar{y}\right| >C
    \middle|
    H_0\text{ is true}
  \right)
  \leq \alpha
  .
\]

Note that $\bar{x}\sim N\left(\mu_1, \sigma_1^2 / n_1\right)$, $\bar{y}\sim N\left(\mu_2, \sigma_2^2 / n_2\right)$ and $\bar{x}\perp\bar{y}$. Therefore
\[
  \bar{x} - \bar{y} \sim N\left(\mu_1 - \mu_2, \sigma_1^2/n_1 + \sigma_2^2 / n_2\right)
  .
\]
Then under $H_0$, $\mu_1 = \mu_2$ and
\[
  \frac{
    \bar{x} - \bar{y}
  }{
    \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}
  }
  \sim
  N\left(0, 1\right)
  ,
\]
which means
\[
  P\left(\left|\bar{x} - \bar{y}\right| > C
    \middle| \mu_1 = \mu_2
  \right)
  = P\left(
    \left|Z\right|
    > \frac{
      C
    }{
      \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}
    }
  \right)
  \leq \alpha
  .
\]
Here to fully utilize the test, we choose the equal sign. Hence
\[
  z_{1 - \alpha / 2} =
  \frac{
    C
  }{
    \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}
  }
  .
\]
Here the reject region is
\[
  \frac{\left|\bar{x} - \bar{y}\right|}{
    \sqrt{
      \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
    }
  }
  > z_{1 - \alpha /2}
  .
\]
\textbf{Note: } Even the samples does not follow normal distribution, by CLT this test still holds true for large sample.

\subsubsection{Power at given $\Delta = \mu_1 - \mu_2$}
\label{sec:power-at-given-2}

The power at a given $\Delta = \mu_1 - \mu_2$ is
\[
  \begin{aligned}
    & P\left(
      \frac{\left|\bar{x} - \bar{y}\right|}{
      \sqrt{
      \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
      }
      }
      > z_{1 - \alpha /2}
      \middle| \Delta = \mu_1 - \mu_2
      \right)    \\
    =& P\left(
       \frac{
       \bar{x} - \bar{y}
       }{
       \sqrt{
       \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
       }
       }
       > z_{1 - \alpha / 2}
       \right)
       + P\left(
       \frac{
       \bar{x} - \bar{y}
       }{
       \sqrt{
       \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
       }
       }
       < z_{\alpha / 2}
       \right)    \\
    =& P\left(
       \frac{
       \bar{x} - \bar{y}
       - \Delta
       }{
       \sqrt{
       \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
       }
       }
       > z_{1 - \alpha / 2}
       - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
       \right)
       + P\left(
       \frac{
       \bar{x} - \bar{y} - \Delta
       }{
       \sqrt{
       \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
       }
       }
       < z_{\alpha / 2}
       - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
       \right)    \\
    =& P\left(
       Z
       > z_{1 - \alpha / 2}
       - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
       \right)
       + P\left(
       Z
       < z_{\alpha / 2}
       - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
       \right)
  \end{aligned}
\]

\subsubsection{Sample size at given $\alpha$, $\beta$, $\Delta$ and $k = \frac{n_1}{n_2}$}
\label{sec:sample-size-at-2}

W.l.o.g, assume $\Delta > 0$, then the power of the test comes mostly from
\[
  P\left(
    Z
    > z_{1 - \alpha / 2}
    - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
  \right)
  .
\]
So to achieve the power, we can set
\[
  P\left(
    Z
    > z_{1 - \alpha / 2}
    - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
  \right)
  \geq 1 -\beta
  .
\]
And this means
\[
  z_{1 - \alpha / 2}
  - \frac{\Delta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
  \leq z_{\beta}
  .
\]
Rearrange this inequality we have
\[
  \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}
  \leq
  \frac{\Delta^2}{
    \left(
      z_{\alpha / 2} + z_{\beta}
    \right)^2
  }
  .
\]

\begin{enumerate}
\item When $n_1$ is fixed and given, we need
  \[
    n_2 \geq
    \frac{
      \sigma_2^2
    }{
      \frac{\Delta^2}{
        \left(
          z_{\alpha / 2} + z_{\beta}
        \right)^2
      }
      - \frac{\sigma_1^2}{n_1}
    }
    .
  \]
  Also this fixed and given $n_1$ must satisfiy
  \[
    \frac{\Delta^2}{
      \left(
        z_{\alpha / 2} + z_{\beta}
      \right)^2
    }
    >
    \frac{\sigma_1^2}{n_1}
  \]
  for the test to be feasible.
\item Similarly, when $n_2$ is given and fixed, we need
  \[
    n_1 \geq
    \frac{\sigma_1^2}{
      \frac{\Delta^2}{
        \left(
          z_{\alpha / 2} + z_{\beta}
        \right)^2
      }
      - \frac{\sigma_2^2}{n_2}
    }
    .
  \]
  Also this fixed and given $n_2$ must satisfy
  \[
        \frac{\Delta^2}{
      \left(
        z_{\alpha / 2} + z_{\beta}
      \right)^2
    }
    >
    \frac{\sigma_2^2}{n_2}
  \]
  for the test to be feasible.
\item For a fixed and given sample size ratio $k = n_2 / n_1$, we need
  \[
    n_1 \geq
    \frac{
      \left(\sigma_1^2 + \sigma_2^2 / k\right)
      \left(
        z_{\alpha / 2} + z_{\beta}
      \right)^2
    }{
      \Delta^2
    }
  \]
\end{enumerate}
\textbf{Note: } These results hold the same form when $\Delta < 0$.

\subsection{Two-sample, variance unknown but equal}
\label{sec:two-sample-variance-1}

When $\sigma_1$ and $\sigma_2$ are both unknown but equal, denoted by $\sigma$. The likelihood of the data becomes
\[
  \begin{aligned}
    & f\left(
      x_1, \cdots, x_{n_1}, y_1, \cdots, y_{n_2}
      \middle|
      \mu_1, \mu_2, \sigma^2
      \right)    \\
    = & \left(
        2\pi\sigma^2
        \right)^{-n_1 / 2 - n_2 / 2}
        \mathrm{exp}\left(
        -\frac{
        \sum\limits_{i = 1}^{n_1}\left(x_i - \mu_1\right)^2
        + \sum\limits_{i = 1}^{n_2}\left(y_i - \mu_2\right)^2
        }{
        2\sigma^2
        }
        \right)
        .
  \end{aligned}
\]
So the MLE under $H_0$ is
\[
  \mu_{1\left(0\right)} = \mu_{2\left(0\right)} = \mu_{\left(0\right)} = \frac{n_1\bar{x} + n_2\bar{y}}{n_1 + n_2}
  ,\quad
  \sigma^2_{\left(0\right)} = \frac{
    \sum\limits_{i = 1}^{n_1}\left(x_i - \mu_{\left(0\right)}\right)^2
    + \sum\limits_{i = 1}^{n_2}\left(y_i  - \mu_{\left(0\right)}\right)^2
  }{
    n_1 + n_2
  }
\]
And the MLE under $H_0\cup H_1$ is
\[
  \mu_{1\left(0\cup1\right)} = \bar{x},\quad \mu_{2\left(0\cup1\right)} = \bar{y}
  ,\quad
  \sigma^2_{\left(0\cup1\right)} =
  \frac{
    \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
    + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2
  }{
    n_1 + n_2
  }
  .
\]
Then the likelihood ratio is
\[
  \begin{aligned}
    LR =& \frac{
          f\left(\bm{x}, \bm{y}\middle|\mu_1 = \mu_2 = \mu_{\left(0\right)}, \sigma^2 = \sigma_{\left(0\right)}^2\right)
          }{
          f\left(\bm{x}, \bm{y}\middle|
          \mu_1 = \mu_{1\left(0\cup1\right)},
          \mu_2 = \mu_{2\left(0\cup1\right)},
          \sigma^2 = \sigma_{\left(0\cup1\right)}^2\right)
          }    \\
    =& \left(
       \frac{\sigma_{\left(0\right)}^2}{\sigma_{\left(0\cup1\right)}^2}
       \right)^{-n_1 / 2 - n_2 / 2}    \\
    =& \left(
       \frac{
       \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x} + \bar{x} - \mu_{\left(0\right)}\right)
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y} + \bar{y} - \mu_{\left(0\right)}\right)
       }{
       \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2
       }
       \right)^{-n_1 / 2 - n_2 / 2}    \\
    =& \left(
       1 + \frac{
       n_1\left(\bar{x} - \mu_{\left(0\right)}\right)^2
       + n_2\left(\bar{y} - \mu_{\left(0\right)}\right)^2
       }{
       \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2
       }
       \right)^{-n_1 / 2 - n_2 / 2}    \\
    =& \left(
       1 + \frac{n_1n_2}{n_1 + n_2}
       \cdot
       \frac{\left(\bar{x} - \bar{y}\right)^2}{
       \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2       
       }
       \right)^{-n_1 / 2 - n_2 / 2}
       .
  \end{aligned}
\]
So to rejact $H_0$ when the likelihood ratio is small enough implies that the reject region is
\[
  \text{Reject region: }
  \left\{
    \left(\bm{x}, \bm{y}\right)
    \middle|
    \frac{\left|\bar{x} - \bar{y}\right|}{
      \sqrt{
               \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2 
      }
    }
    > C
  \right\}
  .
\]

\subsubsection{Decide $C$ from $\alpha$}
\label{sec:decide-c-from-3}

Like before, we know that
\[
  \bar{x} \sim N\left(\mu_1, \frac{\sigma^2}{n_1}\right)
  ,\quad
  \left(n_1 - 1\right)S_x^2 / \sigma^2 \sim \chi^2\left(n_1 - 1\right)
  ,\quad
  \bar{x}\perp S_x^2
  ,
\]
and
\[ 
  \bar{y} \sim N\left(\mu_2, \frac{\sigma^2}{n_2}\right)
  ,\quad
  \left(n_2 - 1\right)S_y^2 / \sigma^2 \sim \chi^2\left(n_2 - 1\right)
  ,\quad
  \bar{y}\perp S_y^2
  .
\]
Since these two samples $\bm{x}$ and $\bm{y}$ are independent, we have
\[
  \bar{x} - \bar{y}
  \sim N\left(
    \mu_1 - \mu_2,\;
    \frac{n_1 + n_2}{n_1n_2}\sigma^2
  \right)
  ,
\]
which implies
\[
  \frac{\bar{x} - \bar{y} - \left(\mu_1 - \mu_2\right)}{
    \sqrt{
      \frac{n_1 + n_2}{n_1n_2}\sigma^2
    }
  }
  \sim N\left(0, 1\right)
  .
\]
And more importantly ({\color{red} the summation of independent $\chi^2$ variables})
\[
  \frac{
    \left(n_1 - 1\right)S_x^2 + \left(n_2 - 1\right)S_y^2
  }{\sigma_2}
  \sim \chi^2\left(n_1 + n_2 - 2\right)
\]
and
\[
    \frac{\bar{x} - \bar{y} - \left(\mu_1 - \mu_2\right)}{
    \sqrt{
      \frac{n_1 + n_2}{n_1n_2}\sigma^2
    }
  }
  \;\bm{\perp}\;
    \frac{
    \left(n_1 - 1\right)S_x^2 + \left(n_2 - 1\right)S_y^2
  }{\sigma_2}
  .
\]
This leads us to
\[
  \frac{
    \frac{\bar{x} - \bar{y} - \left(\mu_1 - \mu_2\right)}{
      \sqrt{
        \frac{n_1 + n_2}{n_1n_2}\sigma^2
      }
    }
  }{
    \sqrt{
      \frac{1}{n_1 + n_2 - 2}
      \frac{
        \left(n_1 - 1\right)S_x^2 + \left(n_2 - 1\right)S_y^2
      }{\sigma_2}
    }
  }
  = \frac{
    \bar{x} - \bar{y} - \left(\mu_1 - \mu_2\right)
  }{\sqrt{
      \left(
        \frac{1}{n_1} + \frac{1}{n_2}
      \right)
      \cdot
      \frac{
        \left(n_1 - 1\right)S_x^2 + \left(n_2 - 1\right)S_y^2
      }{
        n_1 + n_2 - 2
      }
    }
  }
  \sim t(n_1 + n_2 - 2)
  .
\]
Here we use $S_p$ to represent the pooled standard deviation of the data, i.e.
\[
  S_p = \sqrt{
    \frac{
      \left(n_1 - 1\right)S_x^2 + \left(n_2 - 1\right)S_y^2
    }{
      n_1 + n_2 - 2
    }
  }
  .
\]
Under $H_0$, the type-I error is controlled as
\[
  P\left(
        \frac{\left|\bar{x} - \bar{y}\right|}{
      \sqrt{
               \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
       + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2 
      }
    }
    > C
    \middle| \mu_1 = \mu_2
  \right)
  \leq \alpha
  .
\]
Therefore we can write
\[
  P\left(
    \frac{\left|\bar{x} - \bar{y}\right|}{
      \sqrt{
        \sum\limits_{i = 1}^{n_1}\left(x_i - \bar{x}\right)^2
        + \sum\limits_{i = 1}^{n_2}\left(y_i - \bar{y}\right)^2 
      }
    }
    > C
    \middle| \mu_1 = \mu_2
  \right)
  = P\left(
    \left|T_{\left(n_1 + n_2 - 2\right)}\right|
    > \frac{C}{
      \sqrt{\frac{
          \left(\frac{1}{n_1} + \frac{1}{n_2}\right)  
        }{
          n_1 + n_2 - 2
        }
      }
    }
  \right)
  = \alpha
  .
\]
Here in the last part we use the equal sign instead of $\leq$ for fully utilize the test. So we can construct the test to reject $H_0$ when
\[
  \frac{
    \left|\bar{x} - \bar{y}\right|
  }{
    \sqrt{
      \left(\frac{1}{n_1} + \frac{1}{n_2}\right)
      S_p^2
    }
  }
  > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
  .
\]

\subsubsection{Power at given underlying $\Delta = \mu_1 - \mu_2$ and $\sigma^2$}
\label{sec:power-at-given-3}

The distribution of the test statistics is derived as
\[
  \begin{aligned}
    & \frac{
      \bar{x} - \bar{y}
      }{
      \sqrt{
      \left(\frac{1}{n_1} + \frac{1}{n_2}\right)
      S_p^2
      }
      }    \\
    =& \frac{
       \bar{x} - \bar{y}
       - \Delta
       + \Delta
       }{
       \sqrt{
       \left(\frac{1}{n_1} + \frac{1}{n_2}\right)
       S_p^2
       }
       }    \\
    =& \frac{
       \frac{
       \bar{x} - \bar{y} - \Delta
       }{
       \sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}
       }
       + \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}    
       }{
       \sqrt{
       \frac{\left(n_1 + n_2 - 2\right)S_p^2}{\sigma^2}
       \cdot \frac{1}{n_1 + n_2 - 2}
       }
       }
       \sim t(
       n_1 + n_2 - 2,
       \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
       )
       .
  \end{aligned}
\]
So the test statistic follows a non-central t-distribution with degree of freedom $n_1 + n_2 - 2$ and non-central parameter $\frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}$. Then the power of the test
\begin{equation}
  \label{eq:power_equation_two_sample_variance_equal_but_unknown}
    \begin{aligned}
    & P\left(
      \frac{
      \left|\bar{x} - \bar{y}\right|
      }{
      \sqrt{
      \left(\frac{1}{n_1} + \frac{1}{n_2}\right)
      S_p^2
      }
      }
      > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
      \right)    \\
    =& P\left(
       \left|
       T{\left(
       n_1 + n_2 - 2,
       \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
       \right)}
       \right|
       > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
       \right)    \\
    =& P\left(
       T{\left(
       n_1 + n_2 - 2,
       \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
       \right)}
       > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
       \right)    \\
    &+ P\left(
       T{\left(
       n_1 + n_2 - 2,
       \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
       \right)}
       < t_{\alpha / 2}\left(n_1 + n_2 - 2\right)
       \right)    \\
  \end{aligned}
\end{equation}

\subsubsection{Sample size at given $\alpha$, $\beta$, $\Delta$ and $\sigma^2$}
\label{sec:sample-size-at-3}

W.l.o.g, assume $\Delta = \mu_1 - \mu_2 > 0$. Then in the previous power equation~\eqref{eq:power_equation_two_sample_variance_equal_but_unknown},
\[
  P\left(
    T{\left(
        n_1 + n_2 - 2,
        \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
      \right)}
    < t_{\alpha / 2}\left(n_1 + n_2 - 2\right)
  \right)
\]
will be close to zero and
\[
  P\left(
    T{\left(
        n_1 + n_2 - 2,
        \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
      \right)}
    > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
  \right) 
\]
will offer most of the power. So to guarantee a $1 - \beta$ power we can simply set
\[
    P\left(
    T{\left(
        n_1 + n_2 - 2,
        \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
      \right)}
    > t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
  \right)
  \geq 1 - \beta
  .
\]
Therefore
\[
  t_{1 - \alpha / 2}\left(n_1 + n_2 - 2\right)
  \leq
  t_{\beta}\left(
    n_1 + n_2 - 2,
    \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
  \right)
  .
\]
If in another way around $\Delta < 0$, then we set the power inequality
\[
    P\left(
    T{\left(
        n_1 + n_2 - 2,
        \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
      \right)}
    < t_{\alpha / 2}\left(n_1 + n_2 - 2\right)
  \right)
  \geq 1 - \beta
  ,
\]
which means
\[
  t_{\alpha / 2}\left(n_1 + n_2 - 2\right)
  \geq
  t_{1 - \beta}\left(
    n_1 + n_2 - 2,
    \frac{\Delta}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
  \right)
  .
\]
So in summary (using $t_{\alpha}\left(v, \delta\right) + t_{1 - \alpha}\left(v, -\delta\right) = 0$),
\[
  0
  \leq
  t_{\alpha / 2}\left(n_1 + n_2 - 2\right)
  + t_{\beta}\left(
    n_1 + n_2 - 2,
    \frac{\left|\Delta\right|}{\sqrt{\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\sigma^2}}
  \right)
  .
\]

\subsection{Two-sample, variance unknown and unequal}
\label{sec:two-sample-variance-2}







\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
